{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jujun/.conda/envs/jujun_env/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import sys, os\n",
    "cur_path = os.path.join('/research/jujun/text_change')\n",
    "os.chdir(cur_path)\n",
    "\n",
    "import random, pickle\n",
    "import numpy as np\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss, MSELoss\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, f1_score, accuracy_score, precision_recall_fscore_support\n",
    "# import tensorflow as tf\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "from itertools import cycle\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "import datetime\n",
    "# from numba import cuda \n",
    "\n",
    "# from pynvml import *\n",
    "def get_free_gpu():\n",
    "    print('\\n')\n",
    "    # nvmlInit()\n",
    "    # h = nvmlDeviceGetHandleByIndex(0)\n",
    "    # info = nvmlDeviceGetMemoryInfo(h)\n",
    "    # print(f'total    : {info.total // 1024 ** 2}')\n",
    "    # print(f'free     : {info.free// 1024 ** 2}')\n",
    "    # print(f'used     : {info.used// 1024 ** 2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_wordvector(sentences, tokenizer, bert_model, max_len=100):\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    max_len = max_len\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_len,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        #padding='max_length',\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    bert_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(input_ids.to(device), attention_masks.to(device))   \n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    \n",
    "    # get the last four layers\n",
    "    token_embeddings = torch.stack(hidden_states[-4:], dim=0) \n",
    "    #print(token_embeddings.size())\n",
    "\n",
    "    # permute axis\n",
    "    token_embeddings = token_embeddings.permute(1,2,0,3)\n",
    "    #print(token_embeddings.size())\n",
    "\n",
    "    # take the mean of the last 4 layers\n",
    "    token_embeddings = token_embeddings.mean(axis=2)\n",
    "\n",
    "    #print(token_embeddings.size())\n",
    "    input_ids.detach().to('cpu')\n",
    "    attention_masks.detach().to('cpu')\n",
    "    token_embeddings.detach().to('cpu')\n",
    "    del input_ids\n",
    "    return token_embeddings, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(cik, fyear, fyear_bf, tokenizer, bert_model, para_map, para_len, wrd_len=100):\n",
    "    # print(cik, fyear, fyear_bf)\n",
    "    df = pd.concat({k: pd.Series(v) for k, v in para_map[cik].items()})\n",
    "    df = df.reset_index()\n",
    "    df.columns = ['fyear','pid','text']\n",
    "\n",
    "    input = df[df.fyear == fyear].text.values\n",
    "    input_bf = df[df.fyear == fyear_bf].text.values\n",
    "\n",
    "    #get embedding for input\n",
    "    token_embeddings, masks = get_pretrained_wordvector(input, tokenizer, bert_model, max_len = wrd_len)\n",
    "    token_embeddings = token_embeddings.to(device) * masks.unsqueeze(-1).to(device) # (atc_num_para, #wrd_len, #dim)\n",
    "    # padding paragraphs\n",
    "    # print('1 token_embeddings',token_embeddings.size())\n",
    "    pad_num = para_len - token_embeddings.size()[0]\n",
    "    if pad_num>0:\n",
    "        token_embeddings = F.pad(input=token_embeddings, pad=(0,0,0,0,0,pad_num))\n",
    "        # print('2 token_embeddings',token_embeddings.size())\n",
    "    elif pad_num<0:\n",
    "        token_embeddings = token_embeddings[0:para_len]\n",
    "        # print('2 token_embeddings',token_embeddings.size())\n",
    "    else:\n",
    "        token_embeddings = token_embeddings\n",
    "\n",
    "    #get embedding for input_bf\n",
    "    token_embeddings_bf, masks_bf = get_pretrained_wordvector(input_bf, tokenizer, bert_model, max_len = wrd_len)\n",
    "    token_embeddings_bf = token_embeddings_bf.to(device) * masks_bf.unsqueeze(-1).to(device) # (atc_num_para, #wrd_len, #dim)\n",
    "    # padding paragraphs\n",
    "    # print('1 token_embeddings_bf',token_embeddings_bf.size())\n",
    "    pad_num_bf = para_len - token_embeddings_bf.size()[0]\n",
    "    #print('pad_num_bf', pad_num_bf)\n",
    "    if pad_num_bf>0:\n",
    "        # print('>0')\n",
    "        token_embeddings_bf = F.pad(input=token_embeddings_bf, pad=(0,0,0,0,0,pad_num_bf))\n",
    "        # print('2 token_embeddings_bf',token_embeddings_bf.size())\n",
    "    elif pad_num_bf<0:\n",
    "        # print('<0')\n",
    "        token_embeddings_bf = token_embeddings_bf[0:para_len]\n",
    "        # print('2 token_embeddings_bf',token_embeddings_bf.size())\n",
    "    else:\n",
    "        token_embeddings_bf = token_embeddings_bf\n",
    "\n",
    "    return token_embeddings, token_embeddings_bf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "class simple_siamese(nn.Module):\n",
    "    def __init__(self, emb_dim, wrd_len, num_filters, kernel_sizes, kernel_sizes2, num_classes=2.0, dropout_rate = 0.3):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.wrd_len = wrd_len\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.kernel_sizes2 = kernel_sizes2\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv = nn.Sequential(\n",
    "            nn.Conv2d(768, 128, kernel_size = kernel_sizes), # input (#batch, 768, num_para->60, num_words->100) # kernal size = 10,50  # output: (#batch, 256, 50, 50)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, padding=0),  # input (#batch, 256, 50, 50) #output (#batch, 256, 10, 10)\n",
    "            nn.Conv2d(128, 64,  kernel_size = kernel_sizes2), # input (#batch, 256, num_para->10, num_words->10) # kernal size = 3,3\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(2, padding=0),\n",
    "            nn.ReLU(), \n",
    "        )\n",
    "\n",
    "        self.fc = nn.Linear(1152, int(self.num_classes))\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        #permute input to make it fit cnn\n",
    "        x1 = torch.permute(input1, (0,3,1,2))\n",
    "        x2 = torch.permute(input2, (0,3,1,2))\n",
    "        # print(x1.size())\n",
    "        # print(x2.size())\n",
    "\n",
    "        x1 = self.conv(x1)\n",
    "        x2 = self.conv(x2)\n",
    "        x = torch.sub(x1,x2)\n",
    "\n",
    "        \n",
    "        # print(x.size())\n",
    "        x = torch.reshape(x,(x.size()[0],-1))\n",
    "        # print(x.size())\n",
    "        x = self.dropout(x)\n",
    "        logit = self.fc(x)\n",
    "        # print('model output',logit.size())\n",
    "\n",
    "        return logit   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 768\n",
    "wrd_len = 50 #100\n",
    "para_len = 30 #60\n",
    "num_filters = 128\n",
    "kernel_sizes =  (10,10)\n",
    "kernel_sizes2 =  (1,1) #(2,2)\n",
    "dropout_rate = 0.5\n",
    "num_classes=2.0\n",
    "batch_size = 32\n",
    "# para_map = para_map\n",
    "class_weight = 1\n",
    "model = simple_siamese( emb_dim, wrd_len, num_filters, kernel_sizes, \\\n",
    "    kernel_sizes2, num_classes=num_classes,dropout_rate=dropout_rate)\n",
    "#summary(model [(32, 30, 50, 768), (32, 30, 50, 768)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "simple_siamese                           [32, 2]                   --\n",
       "├─Sequential: 1-1                        [32, 64, 3, 6]            --\n",
       "│    └─Conv2d: 2-1                       [32, 128, 21, 41]         9,830,528\n",
       "│    └─ReLU: 2-2                         [32, 128, 21, 41]         --\n",
       "│    └─MaxPool2d: 2-3                    [32, 128, 7, 13]          --\n",
       "│    └─Conv2d: 2-4                       [32, 64, 7, 13]           8,256\n",
       "│    └─ReLU: 2-5                         [32, 64, 7, 13]           --\n",
       "│    └─MaxPool2d: 2-6                    [32, 64, 3, 6]            --\n",
       "│    └─ReLU: 2-7                         [32, 64, 3, 6]            --\n",
       "├─Sequential: 1-2                        [32, 64, 3, 6]            (recursive)\n",
       "│    └─Conv2d: 2-8                       [32, 128, 21, 41]         (recursive)\n",
       "│    └─ReLU: 2-9                         [32, 128, 21, 41]         --\n",
       "│    └─MaxPool2d: 2-10                   [32, 128, 7, 13]          --\n",
       "│    └─Conv2d: 2-11                      [32, 64, 7, 13]           (recursive)\n",
       "│    └─ReLU: 2-12                        [32, 64, 7, 13]           --\n",
       "│    └─MaxPool2d: 2-13                   [32, 64, 3, 6]            --\n",
       "│    └─ReLU: 2-14                        [32, 64, 3, 6]            --\n",
       "├─Dropout: 1-3                           [32, 1152]                --\n",
       "├─Linear: 1-4                            [32, 2]                   2,306\n",
       "==========================================================================================\n",
       "Total params: 9,841,090\n",
       "Trainable params: 9,841,090\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 541.75\n",
       "==========================================================================================\n",
       "Input size (MB): 294.91\n",
       "Forward/backward pass size (MB): 29.70\n",
       "Params size (MB): 39.36\n",
       "Estimated Total Size (MB): 363.98\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(model, [(32, 30, 50, 768), (32, 30, 50, 768)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model, validation_dataloader, num_labels, class_weight=None):\n",
    "    #tokenized_texts = []\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    threshold = 0.5\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        # print('val 1 free gpu',get_free_gpu())\n",
    "        b_input_key = batch[0]\n",
    "        b_labels = batch[1].to(device)\n",
    "\n",
    "\n",
    "        #convert key to text embedding\n",
    "        tk_batch = []\n",
    "        tk_batch_bf = []\n",
    "        #print('val batch',batch)\n",
    "        for t in b_input_key.detach().to('cpu').numpy():\n",
    "            tk, tk_bf = get_text_embedding(t[0], t[1], t[2], tokenizer, bert_model, para_map, para_len, wrd_len=wrd_len)\n",
    "            if tk.size()[0] == para_len:              \n",
    "                tk_batch.append(tk)\n",
    "                tk_batch_bf.append(tk_bf)\n",
    "            else:\n",
    "                print('token size error')\n",
    "                break\n",
    "            \n",
    "\n",
    "        tk_batch = torch.stack(tk_batch)\n",
    "        tk_batch = tk_batch.to(device)\n",
    "\n",
    "        tk_batch_bf = torch.stack(tk_batch_bf)\n",
    "        tk_batch_bf = tk_batch_bf.to(device)\n",
    "        # print('val 2 free gpu',get_free_gpu())\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            logits = model(tk_batch, tk_batch_bf)\n",
    "            #loss_func = BCELoss()\n",
    "            #val_loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "\n",
    "            tk_batch.detach().to('cpu')\n",
    "            del tk_batch\n",
    "            tk_batch_bf.detach().to('cpu')\n",
    "            del tk_batch_bf           \n",
    "            # print('val 3 free gpu',get_free_gpu())\n",
    "            \n",
    "            if class_weight != None:\n",
    "                pos_weight = torch.tensor(class_weight).to(device)\n",
    "                loss_func = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "            else:\n",
    "                loss_func = BCEWithLogitsLoss()\n",
    "\n",
    "            val_loss = loss_func(\n",
    "                logits,\n",
    "                b_labels.type_as(logits))  #convert labels to float for calculation\n",
    "\n",
    "            total_eval_loss += val_loss.item()\n",
    "\n",
    "            pred_label = torch.sigmoid(logits)\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "            pred_label = pred_label.to('cpu').numpy()\n",
    "\n",
    "            #tokenized_texts.append(b_input_ids)\n",
    "            true_labels.append(b_labels)\n",
    "            pred_labels.append(pred_label)\n",
    "\n",
    "    # Flatten outputs\n",
    "    pred_labels = np.vstack(pred_labels)\n",
    "    true_labels = np.vstack(true_labels)\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    return  pred_labels, true_labels, avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, num_labels, para_len, wrd_len, train_dataloader, validation_dataloader, model_path,\\\n",
    "                             optimizer=None, scheduler=None, epochs = 10, \\\n",
    "                             class_weight = None, patience = 5):\n",
    "\n",
    "    seed_val = 42\n",
    "\n",
    "    threshold = 0.5\n",
    "    #model_path = 'best_model.model'  # save the best model\n",
    "\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    training_stats = []\n",
    "\n",
    "    best_score = -0.5\n",
    "    best_epoch = 0\n",
    "    cnt = 0\n",
    "\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    if optimizer == None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "\n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            \n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: (cik, fyear, fyear_bf)\n",
    "            #   [1]: labels\n",
    "            \n",
    "            # print('1 free gpu',get_free_gpu())\n",
    "            b_input_key = batch[0] # batch_size * (cik, fyear, fyear_bf)\n",
    "            b_labels = batch[1].to(device)\n",
    "            \n",
    "            \n",
    "            #convert key to text embedding\n",
    "            tk_batch = []\n",
    "            tk_batch_bf = []\n",
    "            #print('b_input_key',b_input_key)\n",
    "            time_start_tk = time.time()\n",
    "            for t in b_input_key.detach().to('cpu').numpy():\n",
    "                tk, tk_bf = get_text_embedding(t[0], t[1], t[2], tokenizer, bert_model, para_map, para_len, wrd_len=wrd_len)\n",
    "                if tk.size()[0] == para_len:              \n",
    "                    tk_batch.append(tk)\n",
    "                    tk_batch_bf.append(tk_bf)\n",
    "                    # print(len(tk_batch), len(tk_batch_bf))\n",
    "                else:\n",
    "                    print('token size error')\n",
    "                    break\n",
    "            # print(len(tk_batch), len(tk_batch_bf))\n",
    "            # print(\"----- token %s seconds -----\" % (time.time() - time_start_tk))\n",
    "                \n",
    "            tk_batch = torch.stack(tk_batch)\n",
    "            tk_batch = tk_batch.to(device)\n",
    "            \n",
    "            tk_batch_bf = torch.stack(tk_batch_bf)\n",
    "            tk_batch_bf = tk_batch_bf.to(device)\n",
    "            #  print('2 free gpu',get_free_gpu())\n",
    "            model.zero_grad()\n",
    "\n",
    "            time_start_batch_train = time.time()\n",
    "            logits = model(tk_batch,tk_batch_bf)\n",
    "            #print(\"logits shape: \", b_input_ids.size(), b_labels.size(), logits.shape())\n",
    "            #loss_func = BCELoss()\n",
    "            #loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "\n",
    "            # add class weight\n",
    "            if class_weight != None:\n",
    "                pos_weight = torch.tensor(class_weight).to(device)\n",
    "                loss_func = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "            else:\n",
    "                loss_func = BCEWithLogitsLoss()\n",
    "            \n",
    "            tk_batch.detach().to('cpu')\n",
    "            del tk_batch\n",
    "            tk_batch_bf.detach().to('cpu')\n",
    "            del tk_batch_bf\n",
    "            \n",
    "            # print('3 free gpu',get_free_gpu())\n",
    "            # print(logits.size(), b_labels.size())\n",
    "#             loss = loss_func(\n",
    "#                 logits.view(-1, num_labels),\n",
    "#                 b_labels.type_as(logits).view(\n",
    "#                     -1, num_labels))  \n",
    "            # convert labels to float for calculation\n",
    "            loss = loss_func(logits, b_labels.type_as(logits))\n",
    "             \n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate.\n",
    "            if scheduler != None:\n",
    "                scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        training_time = time.time() - t0\n",
    "        print(\"Total training_time took {0:.2f} minutes \".format(training_time/60))\n",
    "\n",
    "            #print(\"\")\n",
    "            #print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "            #print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        testing = False\n",
    "\n",
    "        if testing:\n",
    "            print(\"\")\n",
    "            print(\"Running Validation...\")\n",
    "\n",
    "            t1 = time.time()\n",
    "\n",
    "            # Put the model in evaluation mode--the dropout layers behave differently\n",
    "            # during evaluation.\n",
    "            model.eval()\n",
    "\n",
    "            pred_labels, true_labels, avg_val_loss = model_eval(\n",
    "                model,  validation_dataloader, num_labels, class_weight=class_weight)\n",
    "\n",
    "            pred_bools = np.argmax(pred_labels, axis=1)\n",
    "            true_bools = np.argmax(true_labels, axis=1)\n",
    "\n",
    "            val_f1 = f1_score(true_bools, pred_bools, average=None) * 100\n",
    "            val_f1 = val_f1[1]  # return f1 for  class 1\n",
    "            val_acc = (\n",
    "                pred_bools == true_bools).astype(int).sum() / len(pred_bools)\n",
    "\n",
    "            #print('Validation Accuracy: {0:.4f}, F1: {1:.4f}, Loss: {2:.4f}'.format(val_f1, val_acc, avg_val_loss))\n",
    "            #print(classification_report(np.array(true_labels), pred_bools, target_names=label_cols) )\n",
    "            print(\"Epoch {0}\\t Train Loss: {1:.4f}\\t Val Loss {2:.4f}\\t Val Acc: {3:.4f}\\t Val F1: {4:.4f}\".\\\n",
    "                format(epoch_i +1, avg_train_loss, avg_val_loss, val_acc, val_f1))\n",
    "\n",
    "            # Measure how long the validation run took.\n",
    "            validation_time = time.time() - t1\n",
    "            print(\"Total val_time took {0:.2f} minutes \".format(validation_time/60))\n",
    "\n",
    "            #print(\"  Validation Loss: {0:.2f}\".format(val_f1_accuracy))\n",
    "            #print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "            # Record all statistics from this epoch.\n",
    "            training_stats.append({\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': val_f1,\n",
    "                'Best F1': best_score,\n",
    "                'Best epoch': best_epoch\n",
    "                #'Training Time': training_time,\n",
    "                #'Validation Time': validation_time\n",
    "            })\n",
    "\n",
    "            # early stopping\n",
    "            if val_f1 > best_score:\n",
    "                best_score = val_f1\n",
    "                best_epoch = epoch_i + 1\n",
    "                torch.save(copy.deepcopy(model.state_dict()), model_path)\n",
    "                print(\"model saved\")\n",
    "                cnt = 0\n",
    "            else:\n",
    "                cnt += 1\n",
    "                if cnt == patience:\n",
    "                    print(\"\\n\")\n",
    "                    print(\"early stopping at epoch {0}\".format(epoch_i + 1))\n",
    "                    break\n",
    "\n",
    "            print(\"\")\n",
    "            #print(\"Training complete!\")\n",
    "\n",
    "            print(\"Total training took {0:.2f} minutes\".format((time.time()-total_t0)/60))\n",
    "        else:\n",
    "            training_stats = 0\n",
    "            print(avg_train_loss)\n",
    "        \n",
    "    return model, training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1080 Ti\n",
      "1\n",
      "\n",
      "\n",
      "None\n",
      "successfully load data ...\n",
      "(1674, 5)\n",
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.seq_relationship.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------\n",
      "fraud\n",
      "------------\n",
      "\n",
      "fold 0 \n",
      "\n",
      "train fraud 558 test fraud 279\n",
      "\n",
      "======== Epoch 1 / 20 ========\n",
      "Training...\n",
      "Total training_time took 6.34 minutes \n",
      "2.276280268601009\n",
      "\n",
      "======== Epoch 2 / 20 ========\n",
      "Training...\n",
      "Total training_time took 6.28 minutes \n",
      "0.6967520628656659\n",
      "\n",
      "======== Epoch 3 / 20 ========\n",
      "Training...\n",
      "Total training_time took 6.27 minutes \n",
      "0.6929038081850324\n",
      "\n",
      "======== Epoch 4 / 20 ========\n",
      "Training...\n",
      "Total training_time took 6.30 minutes \n",
      "0.6925166249275208\n",
      "\n",
      "======== Epoch 5 / 20 ========\n",
      "Training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 123\u001b[0m\n\u001b[1;32m    118\u001b[0m model \u001b[39m=\u001b[39m simple_siamese(emb_dim, wrd_len, num_filters, kernel_sizes,\\\n\u001b[1;32m    119\u001b[0m                     kernel_sizes2, num_classes\u001b[39m=\u001b[39mnum_classes, dropout_rate \u001b[39m=\u001b[39m dropout_rate)\n\u001b[1;32m    120\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 123\u001b[0m model, training_stats \u001b[39m=\u001b[39m train_model(model, num_classes, para_len, wrd_len, train_dataloader, validation_dataloader, \\\n\u001b[1;32m    124\u001b[0m                                                 model_path \u001b[39m=\u001b[39;49m model_name, class_weight \u001b[39m=\u001b[39;49m class_weight,\\\n\u001b[1;32m    125\u001b[0m                                                 optimizer\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, scheduler\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, epochs \u001b[39m=\u001b[39;49m \u001b[39m20\u001b[39;49m)\n\u001b[1;32m    127\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mload the best model ... \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    129\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(model_name))\n",
      "Cell \u001b[0;32mIn[8], line 64\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, num_labels, para_len, wrd_len, train_dataloader, validation_dataloader, model_path, optimizer, scheduler, epochs, class_weight, patience)\u001b[0m\n\u001b[1;32m     62\u001b[0m time_start_tk \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     63\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m b_input_key\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mnumpy():\n\u001b[0;32m---> 64\u001b[0m     tk, tk_bf \u001b[39m=\u001b[39m get_text_embedding(t[\u001b[39m0\u001b[39;49m], t[\u001b[39m1\u001b[39;49m], t[\u001b[39m2\u001b[39;49m], tokenizer, bert_model, para_map, para_len, wrd_len\u001b[39m=\u001b[39;49mwrd_len)\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m tk\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m para_len:              \n\u001b[1;32m     66\u001b[0m         tk_batch\u001b[39m.\u001b[39mappend(tk)\n",
      "Cell \u001b[0;32mIn[3], line 11\u001b[0m, in \u001b[0;36mget_text_embedding\u001b[0;34m(cik, fyear, fyear_bf, tokenizer, bert_model, para_map, para_len, wrd_len)\u001b[0m\n\u001b[1;32m      8\u001b[0m input_bf \u001b[39m=\u001b[39m df[df\u001b[39m.\u001b[39mfyear \u001b[39m==\u001b[39m fyear_bf]\u001b[39m.\u001b[39mtext\u001b[39m.\u001b[39mvalues\n\u001b[1;32m     10\u001b[0m \u001b[39m#get embedding for input\u001b[39;00m\n\u001b[0;32m---> 11\u001b[0m token_embeddings, masks \u001b[39m=\u001b[39m get_pretrained_wordvector(\u001b[39minput\u001b[39;49m, tokenizer, bert_model, max_len \u001b[39m=\u001b[39;49m wrd_len)\n\u001b[1;32m     12\u001b[0m token_embeddings \u001b[39m=\u001b[39m token_embeddings\u001b[39m.\u001b[39mto(device) \u001b[39m*\u001b[39m masks\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device) \u001b[39m# (atc_num_para, #wrd_len, #dim)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[39m# padding paragraphs\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[39m# print('1 token_embeddings',token_embeddings.size())\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m, in \u001b[0;36mget_pretrained_wordvector\u001b[0;34m(sentences, tokenizer, bert_model, max_len)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m# For every sentence...\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences:\n\u001b[1;32m      9\u001b[0m \u001b[39m# `encode_plus` will:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m#   (1) Tokenize the sentence.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m#   (5) Pad or truncate the sentence to `max_length`\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m#   (6) Create attention masks for [PAD] tokens.\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     encoded_dict \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m     17\u001b[0m                     sent,                      \u001b[39m# Sentence to encode.\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m                     add_special_tokens \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, \u001b[39m# Add '[CLS]' and '[SEP]'\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m                     max_length \u001b[39m=\u001b[39;49m max_len,           \u001b[39m# Pad & truncate all sentences.\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m                     pad_to_max_length \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     21\u001b[0m                     \u001b[39m#padding='max_length',\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m                     return_attention_mask \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,   \u001b[39m# Construct attn. masks.\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m                     return_tensors \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m,     \u001b[39m# Return pytorch tensors.\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m                )\n\u001b[1;32m     26\u001b[0m     \u001b[39m# Add the encoded sentence to the list.    \u001b[39;00m\n\u001b[1;32m     27\u001b[0m     input_ids\u001b[39m.\u001b[39mappend(encoded_dict[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/.conda/envs/jujun_env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2699\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2689\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2690\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2691\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2692\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2697\u001b[0m )\n\u001b[0;32m-> 2699\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   2700\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2701\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2702\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2703\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2704\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2705\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2706\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2707\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2708\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2709\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2710\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2711\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2712\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2713\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2714\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2715\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2716\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2717\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2718\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/jujun_env/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:502\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_plus\u001b[39m(\n\u001b[1;32m    480\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    481\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    499\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[1;32m    501\u001b[0m     batched_input \u001b[39m=\u001b[39m [(text, text_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [text]\n\u001b[0;32m--> 502\u001b[0m     batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m    503\u001b[0m         batched_input,\n\u001b[1;32m    504\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    505\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    506\u001b[0m         padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    507\u001b[0m         truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    508\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    509\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    510\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    511\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    512\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    513\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    514\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    515\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    516\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m    517\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    518\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    519\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[1;32m    522\u001b[0m     \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    523\u001b[0m     \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    524\u001b[0m     \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/.conda/envs/jujun_env/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:429\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    422\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[1;32m    423\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    426\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    427\u001b[0m )\n\u001b[0;32m--> 429\u001b[0m encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode_batch(\n\u001b[1;32m    430\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    431\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    432\u001b[0m     is_pretokenized\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    433\u001b[0m )\n\u001b[1;32m    435\u001b[0m \u001b[39m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[39m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[39m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[39m#                    ]\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    441\u001b[0m tokens_and_encodings \u001b[39m=\u001b[39m [\n\u001b[1;32m    442\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_encoding(\n\u001b[1;32m    443\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m encodings\n\u001b[1;32m    453\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # If there's a GPU available...\n",
    "    if torch.cuda.is_available():    \n",
    "        # Tell PyTorch to use the GPU. \n",
    "        id = 1 \n",
    "        torch.cuda.set_device(1)\n",
    "        device = torch.device(\"cuda\")\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(id))\n",
    "        print(torch.cuda.current_device())\n",
    "    # If not...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    print(get_free_gpu())\n",
    "\n",
    "    # load data\n",
    "    para_map = pickle.load(open(\"/research/rliu/fraud/data/mda/paragraphs_1994_2016.pkl\",\"rb\"))\n",
    "    pos_neg_pair = pd.read_csv('./data/pos_neg_pair.csv')\n",
    "    pos_neg_pair = pos_neg_pair.dropna()\n",
    "\n",
    "    print('successfully load data ...')\n",
    "\n",
    "    pos_index = pos_neg_pair[pos_neg_pair.fraud == 1].index\n",
    "    neg_index = pos_neg_pair[pos_neg_pair.fraud == 0].sample(len(pos_index)).index\n",
    "    df = pos_neg_pair.loc[neg_index.append(pos_index),:]\n",
    "    print(df.shape)\n",
    "\n",
    "\n",
    "    emb_dim = 768\n",
    "    wrd_len = 50 #100\n",
    "    para_len = 30 #60\n",
    "    num_filters = 128\n",
    "    kernel_sizes =  (10,10)\n",
    "    kernel_sizes2 =  (1,1) #(2,2)\n",
    "    dropout_rate = 0.5\n",
    "    num_classes=2.0\n",
    "    batch_size = 16\n",
    "    para_map = para_map\n",
    "    class_weight = 1\n",
    "\n",
    "    result = []\n",
    "    label_cols = ['fraud']\n",
    "\n",
    "    #embedding\n",
    "    print('Loading BERT tokenizer...')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "\n",
    "    bert_model = AutoModel.from_pretrained(\n",
    "        # 'ProsusAI/finbert',\n",
    "        'bert-base-uncased',\n",
    "        # 'yiyanghkust/finbert-pretrain',\n",
    "        num_labels = 2, \n",
    "        output_attentions = False, # Whether the model returns attentions weights.\n",
    "        output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "        )\n",
    "    bert_model.cuda()\n",
    "\n",
    "    for col in label_cols:\n",
    "        print(\"\\n------------\")\n",
    "        print(col)\n",
    "        print(\"------------\")\n",
    "\n",
    "        y = df[col].astype(int).values\n",
    "        x_key = df[['cik', 'fyear', 'fyear_bf']].values\n",
    "\n",
    "        fold = 0\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=3, random_state=0, shuffle=True)\n",
    "\n",
    "        for train_index, test_index in skf.split(x_key, y):\n",
    "\n",
    "            print(\"\\nfold {} \\n\".format(fold))\n",
    "\n",
    "            fold += 1\n",
    "            X_train, X_test = x_key[train_index], x_key[test_index]\n",
    "            X_train = torch.tensor(X_train)\n",
    "            X_test = torch.tensor(X_test)\n",
    "\n",
    "            Y_train, Y_test = y[train_index], y[test_index]\n",
    "            print('train fraud', sum(Y_train),'test fraud', sum(Y_test))\n",
    "\n",
    "            Y_train = pd.get_dummies(Y_train).values\n",
    "            Y_train = torch.tensor(Y_train)\n",
    "\n",
    "            Y_test = pd.get_dummies(Y_test).values\n",
    "            Y_test = torch.tensor(Y_test)\n",
    "\n",
    "            train_dataset = TensorDataset(X_train, Y_train)\n",
    "            val_dataset = TensorDataset(X_test, Y_test)\n",
    "\n",
    "            train_dataloader = DataLoader(\n",
    "                train_dataset,  # The training samples.\n",
    "                sampler=RandomSampler(train_dataset),  # Select batches randomly\n",
    "                batch_size=batch_size  # Trains with this batch size.\n",
    "            )\n",
    "\n",
    "            validation_dataloader = DataLoader(\n",
    "                val_dataset,  # The validation samples.\n",
    "                sampler=RandomSampler(\n",
    "                    val_dataset),  # Pull out batches sequentially.\n",
    "                batch_size=batch_size  # Evaluate with this batch size.\n",
    "            )\n",
    "\n",
    "            if class_weight == None:\n",
    "                pass\n",
    "            else:\n",
    "                train_sample_weight = np.array(\n",
    "                    [class_weight if i[1] == 1 else 1 for i in Y_train])\n",
    "                test_sample_weight = np.array(\n",
    "                    [class_weight if i[1] == 1 else 1 for i in Y_test])\n",
    "\n",
    "            model_name = \"./model/simple_siamese_\" + str(fold)\n",
    "            #model = cnn(emb_dim, seq_len, num_filters, kernel_sizes, num_labels)\n",
    "            model = simple_siamese(emb_dim, wrd_len, num_filters, kernel_sizes,\\\n",
    "                                kernel_sizes2, num_classes=num_classes, dropout_rate = dropout_rate)\n",
    "            model.to(device)\n",
    "\n",
    "\n",
    "            model, training_stats = train_model(model, num_classes, para_len, wrd_len, train_dataloader, validation_dataloader, \\\n",
    "                                                            model_path = model_name, class_weight = class_weight,\\\n",
    "                                                            optimizer=None, scheduler=None, epochs = 20)\n",
    "\n",
    "            print(\"load the best model ... \")\n",
    "\n",
    "            model.load_state_dict(torch.load(model_name))\n",
    "\n",
    "            # show performance of best model\n",
    "            model.eval()\n",
    "            pred_labels, true_labels,avg_val_loss = model_eval(model, \\\n",
    "                                                    validation_dataloader, num_classes, class_weight = class_weight)\n",
    "\n",
    "            pred_bools = np.argmax(pred_labels, axis = 1)\n",
    "            true_bools = np.argmax(true_labels, axis = 1)\n",
    "\n",
    "            p, r, f, _ = precision_recall_fscore_support(true_bools,pred_bools, pos_label = 1)\n",
    "            #val_f1 = f1_score(true_bools,pred_bools, average = None)*100\n",
    "            #val_f1 = val_f1[1] # return f1 for  class 1\n",
    "            val_acc = (pred_bools == true_bools).astype(int).sum()/len(pred_bools)\n",
    "\n",
    "            print('Precision: {0:.4f}, Recall: {1:.4f}, F1: {2:.4f}, Loss: {3:.4f}'.format(p[1], r[1], f[1], avg_val_loss))\n",
    "            print(classification_report(true_bools, pred_bools) )\n",
    "\n",
    "\n",
    "            result.append([col, fold, p[1], r[1], f[1], training_stats[-1][\"Best epoch\"]])\n",
    "            with open(\"./result/simple_siamese.pkl\", \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(result, fp)\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            get_free_gpu()\n",
    "    print('=== finish  === ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jujun_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d855bc81b0407a867dfce63c14005faefd0874fe81a6ad417f5d7e68a600d1a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
