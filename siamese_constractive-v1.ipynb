{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jujun/.conda/envs/jujun_env/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import sys, os\n",
    "cur_path = os.path.join('/research/jujun/text_change')\n",
    "os.chdir(cur_path)\n",
    "\n",
    "import random, pickle\n",
    "import numpy as np\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss, MSELoss\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, \\\n",
    "                                f1_score, accuracy_score, precision_recall_fscore_support\n",
    "# import tensorflow as tf\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "from itertools import cycle\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "import datetime\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# from numba import cuda \n",
    "\n",
    "# from pynvml import *\n",
    "def get_free_gpu():\n",
    "    print('\\n')\n",
    "    # nvmlInit()\n",
    "    # h = nvmlDeviceGetHandleByIndex(0)\n",
    "    # info = nvmlDeviceGetMemoryInfo(h)\n",
    "    # print(f'total    : {info.total // 1024 ** 2}')\n",
    "    # print(f'free     : {info.free// 1024 ** 2}')\n",
    "    # print(f'used     : {info.used// 1024 ** 2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_wordvector(sentences, tokenizer, bert_model, max_len=100):\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    max_len = max_len\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_len,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        #padding='max_length',\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    bert_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(input_ids.to(device), attention_masks.to(device))   \n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    \n",
    "    # get the last four layers\n",
    "    token_embeddings = torch.stack(hidden_states[-4:], dim=0) \n",
    "    #print(token_embeddings.size())\n",
    "\n",
    "    # permute axis\n",
    "    token_embeddings = token_embeddings.permute(1,2,0,3)\n",
    "    #print(token_embeddings.size())\n",
    "\n",
    "    # take the mean of the last 4 layers\n",
    "    token_embeddings = token_embeddings.mean(axis=2)\n",
    "\n",
    "    #print(token_embeddings.size())\n",
    "    input_ids.detach().to('cpu')\n",
    "    attention_masks.detach().to('cpu')\n",
    "    token_embeddings.detach().to('cpu')\n",
    "    del input_ids\n",
    "    return token_embeddings, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(cik, fyear, fyear_bf, tokenizer, bert_model, para_map, para_len, wrd_len=100):\n",
    "    # print(cik, fyear, fyear_bf)\n",
    "    df = pd.concat({k: pd.Series(v) for k, v in para_map[cik].items()})\n",
    "    df = df.reset_index()\n",
    "    df.columns = ['fyear','pid','text']\n",
    "\n",
    "    input = df[df.fyear == fyear].text.values\n",
    "    input_bf = df[df.fyear == fyear_bf].text.values\n",
    "\n",
    "    #get embedding for input\n",
    "    token_embeddings, masks = get_pretrained_wordvector(input, tokenizer, bert_model, max_len = wrd_len)\n",
    "    token_embeddings = token_embeddings.to(device) * masks.unsqueeze(-1).to(device) # (atc_num_para, #wrd_len, #dim)\n",
    "    # padding paragraphs\n",
    "    # print('1 token_embeddings',token_embeddings.size())\n",
    "    pad_num = para_len - token_embeddings.size()[0]\n",
    "    if pad_num>0:\n",
    "        token_embeddings = F.pad(input=token_embeddings, pad=(0,0,0,0,0,pad_num))\n",
    "        # print('2 token_embeddings',token_embeddings.size())\n",
    "    elif pad_num<0:\n",
    "        token_embeddings = token_embeddings[0:para_len]\n",
    "        # print('2 token_embeddings',token_embeddings.size())\n",
    "    else:\n",
    "        token_embeddings = token_embeddings\n",
    "\n",
    "    #get embedding for input_bf\n",
    "    token_embeddings_bf, masks_bf = get_pretrained_wordvector(input_bf, tokenizer, bert_model, max_len = wrd_len)\n",
    "    token_embeddings_bf = token_embeddings_bf.to(device) * masks_bf.unsqueeze(-1).to(device) # (atc_num_para, #wrd_len, #dim)\n",
    "    # padding paragraphs\n",
    "    # print('1 token_embeddings_bf',token_embeddings_bf.size())\n",
    "    pad_num_bf = para_len - token_embeddings_bf.size()[0]\n",
    "    #print('pad_num_bf', pad_num_bf)\n",
    "    if pad_num_bf>0:\n",
    "        # print('>0')\n",
    "        token_embeddings_bf = F.pad(input=token_embeddings_bf, pad=(0,0,0,0,0,pad_num_bf))\n",
    "        # print('2 token_embeddings_bf',token_embeddings_bf.size())\n",
    "    elif pad_num_bf<0:\n",
    "        # print('<0')\n",
    "        token_embeddings_bf = token_embeddings_bf[0:para_len]\n",
    "        # print('2 token_embeddings_bf',token_embeddings_bf.size())\n",
    "    else:\n",
    "        token_embeddings_bf = token_embeddings_bf\n",
    "\n",
    "    return token_embeddings, token_embeddings_bf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "class simple_siamese(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.emb_dim = config.emb_dim\n",
    "        self.wrd_len = config.wrd_len\n",
    "        self.num_filters = config.num_filters\n",
    "        self.kernel_sizes = config.kernel_sizes\n",
    "        self.kernel_sizes2 = config.kernel_sizes2\n",
    "        self.kernel_sizes3 = config.kernel_sizes3\n",
    "        self.dropout_rate = config.dropout_rate\n",
    "        self.num_classes = config.num_classes\n",
    "        self.test_mode = config.test_mode\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(768, 128, kernel_size = self.kernel_sizes), # input (#batch, 768, num_para->30, num_words->50) # kernal size = 10  # output: (#batch, 128, 30, 40)\n",
    "            nn.Conv2d(128, 64,  kernel_size = self.kernel_sizes2), # input (#batch, 768, num_para->30, num_words->50) # kernal size = 10  # output: (#batch, 128, 30, 40)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((1,3), padding=0),  # input (#batch, 128, 30, 40) #output (#batch, 128, 30, 13)\n",
    "            # nn.MaxPool1d(3, padding=0),  # input (#batch, 128, 30, 40) #output (#batch, 128, 30, 13)\n",
    "            # nn.Conv2d(128, 64,  kernel_size = kernel_sizes2), # input (#batch, 256, num_para->10, num_words->10) # kernal size = 5\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d((1,1), padding=0),\n",
    "            # nn.ReLU(), \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 32,  kernel_size = self.kernel_sizes2), # input (#batch, 768, num_para->30, num_words->50) # kernal size = 10  # output: (#batch, 128, 30, 40)\n",
    "            nn.Conv2d(32, 16,  kernel_size = self.kernel_sizes3),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((2,2), padding=0),\n",
    "        )\n",
    "\n",
    "        \n",
    "        linear_size = 64\n",
    "        self.fc1 = nn.Linear(1056, linear_size)\n",
    "        self.fc2 = nn.Linear(linear_size, int(self.num_classes))\n",
    "        self.norl = nn.BatchNorm1d(linear_size)\n",
    "        self.dropout = nn.Dropout(p=self.dropout_rate)\n",
    "        \n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        #permute input to make it fit cnn\n",
    "        x1 = torch.permute(input1, (0,3,1,2))\n",
    "        x2 = torch.permute(input2, (0,3,1,2))\n",
    "        # print(x1.size())\n",
    "        # print(x2.size())\n",
    "\n",
    "        x1 = self.conv1(x1)\n",
    "        x2 = self.conv1(x2)\n",
    "        if self.test_mode:\n",
    "            print('---conv1 output---')\n",
    "            print(x1.size())\n",
    "            print(x2.size())\n",
    "        x = torch.abs(torch.sub(x1,x2))\n",
    "\n",
    "        \n",
    "        # print(x.size())\n",
    "        x = self.conv2(x)\n",
    "        if self.test_mode:\n",
    "            print('---conv2 output---')\n",
    "            print(x.size())\n",
    "        \n",
    "        x = torch.reshape(x,(x.size()[0],-1))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        # x = self.norl(x)\n",
    "        logit = self.fc2(x)\n",
    "        # print('model output',logit.size())\n",
    "\n",
    "        x1 = torch.reshape(x1,(x1.size()[0],-1))\n",
    "        x2 = torch.reshape(x2,(x2.size()[0],-1))\n",
    "\n",
    "        return logit, x1, x2   \n",
    "        # return x1, x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "global __pred_probs\n",
    "global __labels_bools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1080 Ti\n",
      "1\n"
     ]
    }
   ],
   "source": [
    "class config:\n",
    "    def __init__(self):\n",
    "        self.emb_dim = 768\n",
    "        self.wrd_len = 64  # 100\n",
    "        self.para_len = 32  # 60\n",
    "        self.num_filters = 128\n",
    "        self.kernel_sizes = (1, 10)\n",
    "        self.kernel_sizes2 = (5, 3)  # (2,2)\n",
    "        self.kernel_sizes3 = (3, 3)\n",
    "        self.dropout_rate = 0.2\n",
    "        self.num_classes = 2.0\n",
    "        self.num_labels = 2\n",
    "        self.batch_size = 64\n",
    "        self.para_map = None\n",
    "        self.class_weight = 1\n",
    "        self.test_mode = False\n",
    "\n",
    "    def set_parm_map(self, para_map):\n",
    "        self.para_map = para_map\n",
    "\n",
    "    @staticmethod\n",
    "    def test_model(model, model_config):\n",
    "        # If there's a GPU available...\n",
    "        if torch.cuda.is_available():\n",
    "            # Tell PyTorch to use the GPU.\n",
    "            id = 1\n",
    "            torch.cuda.set_device(1)\n",
    "            device = torch.device(\"cuda\")\n",
    "            print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "            print('We will use the GPU:', torch.cuda.get_device_name(id))\n",
    "            print(torch.cuda.current_device())\n",
    "        # If not...\n",
    "        else:\n",
    "            print('No GPU available, using the CPU instead.')\n",
    "            device = torch.device(\"cpu\")\n",
    "        summary(model, [(model_config.batch_size, model_config.para_len, model_config.wrd_len,\n",
    "                768), (model_config.batch_size, model_config.para_len, model_config.wrd_len, 768)])\n",
    "\n",
    "\n",
    "siamese_config = config()\n",
    "model = simple_siamese(siamese_config)\n",
    "config.test_model(model=model, model_config=siamese_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model, validation_dataloader, num_labels, class_weight=None):\n",
    "    #tokenized_texts = []\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    threshold = 0.5\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        # print('val 1 free gpu',get_free_gpu())\n",
    "        b_input_key = batch[0]\n",
    "        b_labels = batch[1].to(device)\n",
    "\n",
    "\n",
    "        #convert key to text embedding\n",
    "        tk_batch = []\n",
    "        tk_batch_bf = []\n",
    "        #print('val batch',batch)\n",
    "        for t in b_input_key.detach().to('cpu').numpy():\n",
    "            tk, tk_bf = get_text_embedding(t[0], t[1], t[2], tokenizer, bert_model, para_map, para_len, wrd_len=wrd_len)\n",
    "            if tk.size()[0] == para_len:              \n",
    "                tk_batch.append(tk)\n",
    "                tk_batch_bf.append(tk_bf)\n",
    "            else:\n",
    "                print('token size error')\n",
    "                break\n",
    "            \n",
    "\n",
    "        tk_batch = torch.stack(tk_batch)\n",
    "        tk_batch = tk_batch.to(device)\n",
    "\n",
    "        tk_batch_bf = torch.stack(tk_batch_bf)\n",
    "        tk_batch_bf = tk_batch_bf.to(device)\n",
    "        # print('val 2 free gpu',get_free_gpu())\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            logits, x1, x2 = model(tk_batch, tk_batch_bf)\n",
    "            cos_sim = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "            sim = cos_sim(x1,x2)\n",
    "            sim = sim.reshape(-1,1)\n",
    "            #loss_func = BCELoss()\n",
    "            #val_loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "\n",
    "            tk_batch.detach().to('cpu')\n",
    "            del tk_batch\n",
    "            tk_batch_bf.detach().to('cpu')\n",
    "            del tk_batch_bf           \n",
    "            # print('val 3 free gpu',get_free_gpu())\n",
    "            \n",
    "            if class_weight != None:\n",
    "                pos_weight = torch.tensor(class_weight).to(device)\n",
    "                # weights = torch.tensor([pos_weight]).to(device)\n",
    "                ct_loss = nn.CrossEntropyLoss() #weight = weights\n",
    "                loss_func = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "            else:\n",
    "                ct_loss = nn.CrossEntropyLoss()\n",
    "                loss_func = BCEWithLogitsLoss()\n",
    "\n",
    "            global set_ct_loss\n",
    "            if set_ct_loss == True:\n",
    "                val_loss =  loss_func(logits,b_labels.type_as(logits)) \\\n",
    "                    -  ct_loss(sim, torch.argmax(b_labels,axis=1).type_as(sim).reshape(-1,1))  #convert labels to float for calculation\n",
    "            else: \n",
    "                val_loss =  loss_func(logits,b_labels.type_as(logits))\n",
    "\n",
    "            total_eval_loss += val_loss.item()\n",
    "            \n",
    "\n",
    "            pred_label = torch.softmax(logits, dim=1)\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "            pred_label = pred_label.to('cpu').numpy()\n",
    "\n",
    "            #tokenized_texts.append(b_input_ids)\n",
    "            true_labels.append(b_labels)\n",
    "            pred_labels.append(pred_label)\n",
    "\n",
    "    # Flatten outputs\n",
    "    pred_labels = np.vstack(pred_labels)\n",
    "    true_labels = np.vstack(true_labels)\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    return  pred_labels, true_labels, avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, config,  train_dataloader, validation_dataloader, model_path,\\\n",
    "                             optimizer=None, scheduler=None, epochs = 10, \\\n",
    "                             class_weight = None, patience = 5):\n",
    "\n",
    "    seed_val = 1234\n",
    "\n",
    "    threshold = 0.5\n",
    "    #model_path = 'best_model.model'  # save the best model\n",
    "\n",
    "    para_len = config.para_len\n",
    "    wrd_len = config.wrd_len\n",
    "    para_map = config.para_map\n",
    "    class_weight = config.class_weight\n",
    "    num_labels = config.num_labels\n",
    "    verbose_mode = True\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    training_stats = []\n",
    "\n",
    "    best_score = -0.5\n",
    "    best_epoch = 0\n",
    "    cnt = 0\n",
    "\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    if optimizer == None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "\n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        train_true_labels = []\n",
    "        train_pred_labels = []\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            \n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: (cik, fyear, fyear_bf)\n",
    "            #   [1]: labels\n",
    "            \n",
    "            # print('1 free gpu',get_free_gpu())\n",
    "            b_input_key = batch[0] # batch_size * (cik, fyear, fyear_bf)\n",
    "            b_labels = batch[1].to(device)\n",
    "            \n",
    "            \n",
    "            #convert key to text embedding\n",
    "            tk_batch = []\n",
    "            tk_batch_bf = []\n",
    "            #print('b_input_key',b_input_key)\n",
    "            time_start_tk = time.time()\n",
    "            for t in b_input_key.detach().to('cpu').numpy():\n",
    "                tk, tk_bf = get_text_embedding(t[0], t[1], t[2], tokenizer, bert_model, para_map, para_len, wrd_len=wrd_len)\n",
    "                if tk.size()[0] == para_len:              \n",
    "                    tk_batch.append(tk)\n",
    "                    tk_batch_bf.append(tk_bf)\n",
    "                    # print(len(tk_batch), len(tk_batch_bf))\n",
    "                else:\n",
    "                    print('token size error')\n",
    "                    break\n",
    "            # print(len(tk_batch), len(tk_batch_bf))\n",
    "            # print(\"----- token %s seconds -----\" % (time.time() - time_start_tk))\n",
    "                \n",
    "            tk_batch = torch.stack(tk_batch)\n",
    "            tk_batch = tk_batch.to(device)\n",
    "            \n",
    "            tk_batch_bf = torch.stack(tk_batch_bf)\n",
    "            tk_batch_bf = tk_batch_bf.to(device)\n",
    "            #  print('2 free gpu',get_free_gpu())\n",
    "            \n",
    "\n",
    "            time_start_batch_train = time.time()\n",
    "            logits, x1, x2 = model(tk_batch,tk_batch_bf)\n",
    "            cos_sim = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "            sim = cos_sim(x1,x2)\n",
    "            sim = sim.reshape(-1,1)\n",
    "            #print(\"logits shape: \", b_input_ids.size(), b_labels.size(), logits.shape())\n",
    "            #loss_func = BCELoss()\n",
    "            #loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "\n",
    "            # add class weight\n",
    "            if class_weight != None:\n",
    "                pos_weight = torch.tensor(class_weight).to(device)\n",
    "                weights = torch.tensor([pos_weight]).to(device)\n",
    "                ct_loss = nn.CrossEntropyLoss()#weight = weights\n",
    "                loss_func = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "            else:\n",
    "                ct_loss = nn.CrossEntropyLoss()\n",
    "                loss_func = BCEWithLogitsLoss()\n",
    "            \n",
    "            tk_batch.detach().to('cpu')\n",
    "            del tk_batch\n",
    "            tk_batch_bf.detach().to('cpu')\n",
    "            del tk_batch_bf\n",
    "            \n",
    "            # print('3 free gpu',get_free_gpu())\n",
    "            # print(logits.size(), b_labels.size())\n",
    "#             loss = loss_func(\n",
    "#                 logits.view(-1, num_labels),\n",
    "#                 b_labels.type_as(logits).view(\n",
    "#                     -1, num_labels))  \n",
    "            # convert labels to float for calculation\n",
    "            # global my_ct_loss, my_sim, my_label\n",
    "            # my_sim = sim\n",
    "            # my_label = b_labels\n",
    "            # my_ct_loss = ct_loss(sim, torch.argmax(b_labels,axis=1).type_as(logits).reshape(-1,1)) \n",
    "            global set_ct_loss\n",
    "            global lista \n",
    "\n",
    "            if verbose_mode:\n",
    "                print(\"logits: \", logits)\n",
    "                print(\"b_labels.type_as(logits): \", b_labels.type_as(logits))\n",
    "                \n",
    "                train_pred_bools = torch.argmax(logits, axis=1)\n",
    "                train_pred_bools = train_pred_bools.to('cpu').numpy()\n",
    "                train_true_bools = torch.argmax(b_labels.type_as(logits), axis=1)\n",
    "                train_true_bools = train_true_bools.to('cpu').numpy()\n",
    "                # print(train_pred_bools.shape, train_true_bools.shape)\n",
    "\n",
    "                train_true_labels += train_true_bools.tolist()\n",
    "                train_pred_labels += train_pred_bools.tolist()\n",
    "                print(\"train_pred_bools\", train_pred_bools)\n",
    "                print(\"train_true_bools\", train_true_bools)\n",
    "                \n",
    "\n",
    "            if set_ct_loss == True:\n",
    "                loss =  loss_func(logits,b_labels.type_as(logits)) \\\n",
    "                    -  ct_loss(sim, torch.argmax(b_labels,axis=1).type_as(sim).reshape(-1,1))  #convert labels to float for calculation\n",
    "            else: \n",
    "                loss =  loss_func(logits, b_labels.type_as(logits))\n",
    "\n",
    "            total_train_loss += loss.item()\n",
    "            # print(f\"train step loss: {step} -- {loss}\")\n",
    "            # print(f\"train step total_train_loss: {step} -- {total_train_loss}\")\n",
    "\n",
    "            model.zero_grad()\n",
    "            \n",
    "            loss.backward()\n",
    "\n",
    "            # torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate.\n",
    "            if scheduler != None:\n",
    "                scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        training_time = time.time() - t0\n",
    "        print(\"Total training_time took {0:.2f} minutes \".format(training_time/60))\n",
    "\n",
    "        # calculate the total accrurcy in this epoch\n",
    "        # print(train_true_labels[0:1])\n",
    "        global lista\n",
    "        global listb\n",
    "        lista = train_true_labels\n",
    "        listb = train_pred_labels\n",
    "        train_true_labels =  np.array(train_true_labels)\n",
    "        train_pred_labels = np.array(train_pred_labels)\n",
    "        print('training acc', (train_true_labels == train_pred_labels).sum(),len(train_true_labels) )\n",
    "        train_acc = (train_true_labels == train_pred_labels).sum()/len(train_true_labels)\n",
    "\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        testing = True\n",
    "\n",
    "        if testing:\n",
    "            print(\"\")\n",
    "            print(\"Running Validation...\")\n",
    "\n",
    "            t1 = time.time()\n",
    "\n",
    "            # Put the model in evaluation mode--the dropout layers behave differently\n",
    "            # during evaluation.\n",
    "            model.eval()\n",
    "\n",
    "            pred_labels, true_labels, avg_val_loss = model_eval(\n",
    "                model,  validation_dataloader, num_labels, class_weight=class_weight)\n",
    "\n",
    "            global val_label_save\n",
    "            global val_true_label_save\n",
    "            val_label_save.append(pred_labels)\n",
    "            val_true_label_save.append(true_labels)\n",
    "\n",
    "\n",
    "            pred_bools = np.argmax(pred_labels, axis=1)\n",
    "            true_bools = np.argmax(true_labels, axis=1)\n",
    "\n",
    "            val_f1 = f1_score(true_bools, pred_bools, average=None) * 100\n",
    "            val_f1 = val_f1[1]  # return f1 for  class 1\n",
    "            val_acc = (pred_bools == true_bools).astype(int).sum() / len(pred_bools)\n",
    "            val_auc = roc_auc_score(true_bools, pred_labels[:,1])\n",
    "\n",
    "            #print('Validation Accuracy: {0:.4f}, F1: {1:.4f}, Loss: {2:.4f}'.format(val_f1, val_acc, avg_val_loss))\n",
    "            #print(classification_report(np.array(true_labels), pred_bools, target_names=label_cols) )\n",
    "            print(\"Epoch {0}\\t Train Loss: {1:.4f}\\t  Train ACC: {2:.4f}\\t Val Loss {3:.4f}\\t Val Acc: {4:.4f}\\t Val F1: {5:.4f}\\t Val AUC: {6:.4f}\".\\\n",
    "                format(epoch_i +1, avg_train_loss, train_acc, avg_val_loss, val_acc, val_f1, val_auc))\n",
    "\n",
    "            # Measure how long the validation run took.\n",
    "            validation_time = time.time() - t1\n",
    "            print(\"Total val_time took {0:.2f} minutes \".format(validation_time/60))\n",
    "\n",
    "            #print(\"  Validation Loss: {0:.2f}\".format(val_f1_accuracy))\n",
    "            #print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "            # Record all statistics from this epoch.\n",
    "            training_stats.append({\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': val_f1,\n",
    "                'Valid. AUC':val_auc,\n",
    "                'Best F1': best_score,\n",
    "                'Best epoch': best_epoch\n",
    "                #'Training Time': training_time,\n",
    "                #'Validation Time': validation_time\n",
    "            })\n",
    "\n",
    "            # early stopping\n",
    "            if val_f1 > best_score:\n",
    "                best_score = val_f1\n",
    "                best_epoch = epoch_i + 1\n",
    "                torch.save(copy.deepcopy(model.state_dict()), model_path)\n",
    "                print(\"model saved\")\n",
    "                cnt = 0\n",
    "            else:\n",
    "                cnt += 1\n",
    "                if cnt == patience:\n",
    "                    print(\"\\n\")\n",
    "                    print(\"early stopping at epoch {0}\".format(epoch_i + 1))\n",
    "                    break\n",
    "\n",
    "            print(\"\")\n",
    "            #print(\"Training complete!\")\n",
    "\n",
    "            print(\"Total training took {0:.2f} minutes\".format((time.time()-total_t0)/60))\n",
    "        else:\n",
    "            training_stats = 0\n",
    "            print(avg_train_loss)\n",
    "        \n",
    "    return model, training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'lista' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m lista\n",
      "\u001b[0;31mNameError\u001b[0m: name 'lista' is not defined"
     ]
    }
   ],
   "source": [
    "lista"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(lista)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "67"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(listb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5074626865671642"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(np.array(lista) == np.array(listb)).sum()/len(listb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3543, 0.6457],\n",
       "        [0.4256, 0.5744]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([[1.2,1.8],[0.4,0.7]]),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "18"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_true_label_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_label_save[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(200,)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = val_label_save[0].sum(axis=1)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 0.99999994, 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.99999994, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 0.99999994,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 0.99999994, 0.99999994,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 0.99999994, 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ,\n",
       "       0.99999994, 1.        , 1.        , 1.        , 1.        ,\n",
       "       1.        , 1.        , 1.        , 1.        , 1.        ],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = val_label_save[-6][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.5232506 , 0.52386016, 0.5344516 , 0.53550726, 0.5339036 ,\n",
       "       0.5286611 , 0.54368126, 0.52959067, 0.53475684, 0.5256853 ,\n",
       "       0.5267108 , 0.5245599 , 0.5256226 , 0.53362614, 0.5177412 ,\n",
       "       0.5290738 , 0.529948  , 0.52800107, 0.535685  , 0.5296777 ,\n",
       "       0.5218666 , 0.5270513 , 0.5201257 , 0.53249747, 0.52939105,\n",
       "       0.5444209 , 0.52398705, 0.52389795, 0.52814114, 0.5319519 ,\n",
       "       0.542003  , 0.52385616, 0.5255244 , 0.54056376, 0.53032464,\n",
       "       0.5200995 , 0.5266097 , 0.5445139 , 0.5436612 , 0.53760165,\n",
       "       0.52690154, 0.5336137 , 0.52661014, 0.5175091 , 0.5360116 ,\n",
       "       0.5146469 , 0.5444209 , 0.52182204, 0.52531785, 0.53059375,\n",
       "       0.52379906, 0.5237532 , 0.52738565, 0.537102  , 0.5298776 ,\n",
       "       0.5313965 , 0.5444209 , 0.5305427 , 0.53344834, 0.5347967 ,\n",
       "       0.5347793 , 0.53182554, 0.52058893, 0.5248482 , 0.5445116 ,\n",
       "       0.53187764, 0.52167225, 0.5279042 , 0.5444209 , 0.52309513,\n",
       "       0.5233589 , 0.526377  , 0.52767414, 0.53171754, 0.5278243 ,\n",
       "       0.5236914 , 0.53111106, 0.53145856, 0.53837997, 0.54021734,\n",
       "       0.52733415, 0.5444209 , 0.53894365, 0.5264531 , 0.52539766,\n",
       "       0.53347206, 0.5234432 , 0.52611053, 0.5307614 , 0.5311188 ,\n",
       "       0.5289063 , 0.5386362 , 0.5272374 , 0.5269312 , 0.5444209 ,\n",
       "       0.53484106, 0.5231909 , 0.53432584, 0.5374232 , 0.5363107 ,\n",
       "       0.52896607, 0.5376783 , 0.533387  , 0.5415936 , 0.5412273 ,\n",
       "       0.52701706, 0.52021104, 0.5239231 , 0.52942353, 0.54245305,\n",
       "       0.529255  , 0.53740704, 0.52882856, 0.533306  , 0.5441272 ,\n",
       "       0.5252909 , 0.52819705, 0.53995603, 0.5444209 , 0.53343064,\n",
       "       0.5361816 , 0.5289746 , 0.5301317 , 0.5172907 , 0.5324675 ,\n",
       "       0.5292827 , 0.5371292 , 0.53039557, 0.53041834, 0.5337891 ,\n",
       "       0.5444209 , 0.52287215, 0.52436435, 0.52656436, 0.5291967 ,\n",
       "       0.5348332 , 0.5428467 , 0.5214875 , 0.5444209 , 0.52779615,\n",
       "       0.5243266 , 0.5441867 , 0.5273261 , 0.53619254, 0.5222131 ,\n",
       "       0.52277625, 0.54400104, 0.53379697, 0.52685755, 0.5313955 ,\n",
       "       0.5437501 , 0.5254951 , 0.5351441 , 0.5294371 , 0.51996577,\n",
       "       0.5279399 , 0.5278855 , 0.53106725, 0.52615833, 0.5378316 ,\n",
       "       0.53071535, 0.536214  , 0.5286343 , 0.52578634, 0.53091645,\n",
       "       0.53429735, 0.5231423 , 0.52890855, 0.528456  , 0.53170615,\n",
       "       0.527932  , 0.52771556, 0.528957  , 0.52195334, 0.5333441 ,\n",
       "       0.5264274 , 0.5238866 , 0.524075  , 0.53413516, 0.54486275,\n",
       "       0.524945  , 0.52664983, 0.52691776, 0.5444209 , 0.5234715 ,\n",
       "       0.5202584 , 0.5346228 , 0.53535867, 0.5267555 , 0.5275513 ,\n",
       "       0.5312791 , 0.5365671 , 0.53975284, 0.5220513 , 0.52181315,\n",
       "       0.52501565, 0.5327609 , 0.54064333, 0.5313472 , 0.52228236],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a>0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "para_map = pickle.load(open(\"/research/rliu/fraud/data/mda/paragraphs_1994_2016.pkl\",\"rb\"))\n",
    "pos_neg_pair = pd.read_csv('./data/pos_neg_pair.csv')\n",
    "pos_neg_pair = pos_neg_pair.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75208      12\n",
       "803014     10\n",
       "849547     10\n",
       "6284       10\n",
       "859475     10\n",
       "           ..\n",
       "928395      1\n",
       "932112      1\n",
       "18498       1\n",
       "947431      1\n",
       "1604028     1\n",
       "Name: cik, Length: 328, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_neg_pair[pos_neg_pair.fraud == 1].cik.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>fyear</th>\n",
       "      <th>count_para</th>\n",
       "      <th>fraud</th>\n",
       "      <th>fyear_bf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150242</th>\n",
       "      <td>1604028</td>\n",
       "      <td>2015</td>\n",
       "      <td>183</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150243</th>\n",
       "      <td>1604028</td>\n",
       "      <td>2016</td>\n",
       "      <td>152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            cik  fyear  count_para  fraud  fyear_bf\n",
       "150242  1604028   2015         183    1.0    2014.0\n",
       "150243  1604028   2016         152    0.0    2015.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_neg_pair[pos_neg_pair.cik == 1604028]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cik = list(set(pos_neg_pair[pos_neg_pair.fraud == 1].cik))\n",
    "neg_cik = list(set(pos_neg_pair[pos_neg_pair.fraud == 0].cik))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_cik = [c for c in neg_cik if c not in pos_cik]\n",
    "neg_cik = random.sample(neg_cik, len(pos_cik))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328 328\n"
     ]
    }
   ],
   "source": [
    "print(len(neg_cik), len(pos_cik))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "656"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_cik + neg_cik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully load data ...\n",
      "(6293, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('successfully load data ...')\n",
    "df = pos_neg_pair[pos_neg_pair.cik.isin(pos_cik + neg_cik)]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "656"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df.cik))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'my_label_1' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m my_label_1\n",
      "\u001b[0;31mNameError\u001b[0m: name 'my_label_1' is not defined"
     ]
    }
   ],
   "source": [
    "my_label_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tb = pd.DataFrame(result, columns=['label','fold','precison','recall','f1','acc','auc','best_epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weight = 10\n",
    "weights = torch.tensor([1,pos_weight]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_loss = nn.CrossEntropyLoss(weight = weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_loss = nn.CrossEntropyLoss(weight = weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0]], device='cuda:1', dtype=torch.uint8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jujun_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d855bc81b0407a867dfce63c14005faefd0874fe81a6ad417f5d7e68a600d1a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
