{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jujun/.conda/envs/jujun_env/lib/python3.11/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# initialization\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "# from IPython.core.interactiveshell import InteractiveShell\n",
    "# InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "import sys, os\n",
    "cur_path = os.path.join('/research/jujun/text_change')\n",
    "os.chdir(cur_path)\n",
    "\n",
    "import random, pickle\n",
    "import numpy as np\n",
    "from torch.nn import BCEWithLogitsLoss, BCELoss, MSELoss\n",
    "from sklearn.metrics import classification_report, confusion_matrix, multilabel_confusion_matrix, \\\n",
    "                                f1_score, accuracy_score, precision_recall_fscore_support\n",
    "# import tensorflow as tf\n",
    "import torch\n",
    "import pandas as pd\n",
    "from transformers import AutoConfig, AutoModel, AutoTokenizer, AutoModelForSequenceClassification\n",
    "import torch.nn as nn\n",
    "from torchinfo import summary\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, random_split\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold\n",
    "from sklearn.utils import shuffle\n",
    "from itertools import cycle\n",
    "from tqdm import tqdm\n",
    "import time\n",
    "import copy\n",
    "import datetime\n",
    "from sklearn.metrics import roc_auc_score\n",
    "# from numba import cuda \n",
    "\n",
    "# from pynvml import *\n",
    "def get_free_gpu():\n",
    "    print('\\n')\n",
    "    # nvmlInit()\n",
    "    # h = nvmlDeviceGetHandleByIndex(0)\n",
    "    # info = nvmlDeviceGetMemoryInfo(h)\n",
    "    # print(f'total    : {info.total // 1024 ** 2}')\n",
    "    # print(f'free     : {info.free// 1024 ** 2}')\n",
    "    # print(f'used     : {info.used// 1024 ** 2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pretrained_wordvector(sentences, tokenizer, bert_model, max_len=100):\n",
    "\n",
    "    input_ids = []\n",
    "    attention_masks = []\n",
    "    max_len = max_len\n",
    "\n",
    "    # For every sentence...\n",
    "    for sent in sentences:\n",
    "    # `encode_plus` will:\n",
    "    #   (1) Tokenize the sentence.\n",
    "    #   (2) Prepend the `[CLS]` token to the start.\n",
    "    #   (3) Append the `[SEP]` token to the end.\n",
    "    #   (4) Map tokens to their IDs.\n",
    "    #   (5) Pad or truncate the sentence to `max_length`\n",
    "    #   (6) Create attention masks for [PAD] tokens.\n",
    "        encoded_dict = tokenizer.encode_plus(\n",
    "                        sent,                      # Sentence to encode.\n",
    "                        add_special_tokens = True, # Add '[CLS]' and '[SEP]'\n",
    "                        max_length = max_len,           # Pad & truncate all sentences.\n",
    "                        pad_to_max_length = True,\n",
    "                        #padding='max_length',\n",
    "                        return_attention_mask = True,   # Construct attn. masks.\n",
    "                        return_tensors = 'pt',     # Return pytorch tensors.\n",
    "                   )\n",
    "    \n",
    "        # Add the encoded sentence to the list.    \n",
    "        input_ids.append(encoded_dict['input_ids'])\n",
    "    \n",
    "        # And its attention mask (simply differentiates padding from non-padding).\n",
    "        attention_masks.append(encoded_dict['attention_mask'])\n",
    "\n",
    "\n",
    "    # Convert the lists into tensors.\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "    attention_masks = torch.cat(attention_masks, dim=0)\n",
    "\n",
    "    bert_model.eval()\n",
    "    with torch.no_grad():\n",
    "        outputs = bert_model(input_ids.to(device), attention_masks.to(device))   \n",
    "        hidden_states = outputs[2]\n",
    "\n",
    "    \n",
    "    # get the last four layers\n",
    "    token_embeddings = torch.stack(hidden_states[-4:], dim=0) \n",
    "    #print(token_embeddings.size())\n",
    "\n",
    "    # permute axis\n",
    "    token_embeddings = token_embeddings.permute(1,2,0,3)\n",
    "    #print(token_embeddings.size())\n",
    "\n",
    "    # take the mean of the last 4 layers\n",
    "    token_embeddings = token_embeddings.mean(axis=2)\n",
    "\n",
    "    #print(token_embeddings.size())\n",
    "    input_ids.detach().to('cpu')\n",
    "    attention_masks.detach().to('cpu')\n",
    "    token_embeddings.detach().to('cpu')\n",
    "    del input_ids\n",
    "    return token_embeddings, attention_masks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_text_embedding(cik, fyear, fyear_bf, tokenizer, bert_model, para_map, para_len, wrd_len=100):\n",
    "    # print(cik, fyear, fyear_bf)\n",
    "    df = pd.concat({k: pd.Series(v) for k, v in para_map[cik].items()})\n",
    "    df = df.reset_index()\n",
    "    df.columns = ['fyear','pid','text']\n",
    "\n",
    "    input = df[df.fyear == fyear].text.values\n",
    "    input_bf = df[df.fyear == fyear_bf].text.values\n",
    "\n",
    "    #get embedding for input\n",
    "    token_embeddings, masks = get_pretrained_wordvector(input, tokenizer, bert_model, max_len = wrd_len)\n",
    "    token_embeddings = token_embeddings.to(device) * masks.unsqueeze(-1).to(device) # (atc_num_para, #wrd_len, #dim)\n",
    "    # padding paragraphs\n",
    "    # print('1 token_embeddings',token_embeddings.size())\n",
    "    pad_num = para_len - token_embeddings.size()[0]\n",
    "    if pad_num>0:\n",
    "        token_embeddings = F.pad(input=token_embeddings, pad=(0,0,0,0,0,pad_num))\n",
    "        # print('2 token_embeddings',token_embeddings.size())\n",
    "    elif pad_num<0:\n",
    "        token_embeddings = token_embeddings[0:para_len]\n",
    "        # print('2 token_embeddings',token_embeddings.size())\n",
    "    else:\n",
    "        token_embeddings = token_embeddings\n",
    "\n",
    "    #get embedding for input_bf\n",
    "    token_embeddings_bf, masks_bf = get_pretrained_wordvector(input_bf, tokenizer, bert_model, max_len = wrd_len)\n",
    "    token_embeddings_bf = token_embeddings_bf.to(device) * masks_bf.unsqueeze(-1).to(device) # (atc_num_para, #wrd_len, #dim)\n",
    "    # padding paragraphs\n",
    "    # print('1 token_embeddings_bf',token_embeddings_bf.size())\n",
    "    pad_num_bf = para_len - token_embeddings_bf.size()[0]\n",
    "    #print('pad_num_bf', pad_num_bf)\n",
    "    if pad_num_bf>0:\n",
    "        # print('>0')\n",
    "        token_embeddings_bf = F.pad(input=token_embeddings_bf, pad=(0,0,0,0,0,pad_num_bf))\n",
    "        # print('2 token_embeddings_bf',token_embeddings_bf.size())\n",
    "    elif pad_num_bf<0:\n",
    "        # print('<0')\n",
    "        token_embeddings_bf = token_embeddings_bf[0:para_len]\n",
    "        # print('2 token_embeddings_bf',token_embeddings_bf.size())\n",
    "    else:\n",
    "        token_embeddings_bf = token_embeddings_bf\n",
    "\n",
    "    return token_embeddings, token_embeddings_bf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define model\n",
    "class simple_siamese(nn.Module):\n",
    "    def __init__(self, emb_dim, wrd_len, num_filters, kernel_sizes, kernel_sizes2, num_classes=2.0, dropout_rate = 0.3):\n",
    "        super().__init__()\n",
    "        self.emb_dim = emb_dim\n",
    "        self.wrd_len = wrd_len\n",
    "        self.num_filters = num_filters\n",
    "        self.kernel_sizes = kernel_sizes\n",
    "        self.kernel_sizes2 = kernel_sizes2\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.num_classes = num_classes\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv2d(768, 128, kernel_size = kernel_sizes), # input (#batch, 768, num_para->30, num_words->50) # kernal size = 10  # output: (#batch, 128, 30, 40)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((1,3), padding=0),  # input (#batch, 128, 30, 40) #output (#batch, 128, 30, 13)\n",
    "            # nn.Conv2d(128, 64,  kernel_size = kernel_sizes2), # input (#batch, 256, num_para->10, num_words->10) # kernal size = 5\n",
    "            # nn.ReLU(inplace=True),\n",
    "            # nn.MaxPool2d((1,1), padding=0),\n",
    "            # nn.ReLU(), \n",
    "        )\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv2d(128, 64,  kernel_size = kernel_sizes2), # input (#batch, 768, num_para->30, num_words->50) # kernal size = 10  # output: (#batch, 128, 30, 40)\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d((1,1), padding=0),\n",
    "            nn.ReLU()\n",
    "        )\n",
    "        self.fc1 = nn.Linear(18304, 9152)\n",
    "        self.fc2 = nn.Linear(9152, int(self.num_classes))\n",
    "        self.norl = nn.BatchNorm1d(9152)\n",
    "        self.dropout = nn.Dropout(p=dropout_rate)\n",
    "        \n",
    "\n",
    "    def forward(self, input1, input2):\n",
    "        #permute input to make it fit cnn\n",
    "        x1 = torch.permute(input1, (0,3,1,2))\n",
    "        x2 = torch.permute(input2, (0,3,1,2))\n",
    "        # print(x1.size())\n",
    "        # print(x2.size())\n",
    "\n",
    "        x1 = self.conv1(x1)\n",
    "        x2 = self.conv1(x2)\n",
    "        # print('---')\n",
    "        # print(x1.size())\n",
    "        # print(x2.size())\n",
    "        x = torch.sub(x1,x2)\n",
    "\n",
    "        \n",
    "        # print(x.size())\n",
    "        x = self.conv2(x)\n",
    "        #  print(x.size())\n",
    "        x = torch.reshape(x,(x.size()[0],-1))\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.norl(x)\n",
    "        logit = self.fc2(x)\n",
    "        # print('model output',logit.size())\n",
    "\n",
    "        x1 = torch.reshape(x1,(x1.size()[0],-1))\n",
    "        x2 = torch.reshape(x2,(x2.size()[0],-1))\n",
    "\n",
    "        return logit, x1, x2   \n",
    "        # return x1, x2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "global __pred_probs\n",
    "global __labels_bools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "emb_dim = 768\n",
    "wrd_len = 50 #100\n",
    "para_len = 30 #60\n",
    "num_filters = 128\n",
    "kernel_sizes =  (1,10)\n",
    "kernel_sizes2 =  (5,3) #(2,2)\n",
    "dropout_rate = 0.5\n",
    "num_classes=2.0\n",
    "batch_size = 32\n",
    "# para_map = para_map\n",
    "class_weight = 1\n",
    "model = simple_siamese( emb_dim, wrd_len, num_filters, kernel_sizes, \\\n",
    "    kernel_sizes2, num_classes=num_classes,dropout_rate=dropout_rate)\n",
    "#summary(model [(32, 30, 50, 768), (32, 30, 50, 768)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1080 Ti\n",
      "1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "simple_siamese                           [32, 2]                   --\n",
       "├─Sequential: 1-1                        [32, 128, 30, 13]         --\n",
       "│    └─Conv2d: 2-1                       [32, 128, 30, 41]         983,168\n",
       "│    └─ReLU: 2-2                         [32, 128, 30, 41]         --\n",
       "│    └─MaxPool2d: 2-3                    [32, 128, 30, 13]         --\n",
       "├─Sequential: 1-2                        [32, 128, 30, 13]         (recursive)\n",
       "│    └─Conv2d: 2-4                       [32, 128, 30, 41]         (recursive)\n",
       "│    └─ReLU: 2-5                         [32, 128, 30, 41]         --\n",
       "│    └─MaxPool2d: 2-6                    [32, 128, 30, 13]         --\n",
       "├─Sequential: 1-3                        [32, 64, 26, 11]          --\n",
       "│    └─Conv2d: 2-7                       [32, 64, 26, 11]          122,944\n",
       "│    └─ReLU: 2-8                         [32, 64, 26, 11]          --\n",
       "│    └─MaxPool2d: 2-9                    [32, 64, 26, 11]          --\n",
       "│    └─ReLU: 2-10                        [32, 64, 26, 11]          --\n",
       "├─Dropout: 1-4                           [32, 18304]               --\n",
       "├─Linear: 1-5                            [32, 9152]                167,527,360\n",
       "├─Dropout: 1-6                           [32, 9152]                --\n",
       "├─BatchNorm1d: 1-7                       [32, 9152]                18,304\n",
       "├─Linear: 1-8                            [32, 2]                   18,306\n",
       "==========================================================================================\n",
       "Total params: 168,670,082\n",
       "Trainable params: 168,670,082\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (G): 83.88\n",
       "==========================================================================================\n",
       "Input size (MB): 294.91\n",
       "Forward/backward pass size (MB): 49.68\n",
       "Params size (MB): 674.68\n",
       "Estimated Total Size (MB): 1019.27\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If there's a GPU available...\n",
    "if torch.cuda.is_available():    \n",
    "    # Tell PyTorch to use the GPU. \n",
    "    id = 1 \n",
    "    torch.cuda.set_device(1)\n",
    "    device = torch.device(\"cuda\")\n",
    "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(id))\n",
    "    print(torch.cuda.current_device())\n",
    "# If not...\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "summary(model, [(32, 30, 50, 768), (32, 30, 50, 768)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "167490752"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_eval(model, validation_dataloader, num_labels, class_weight=None):\n",
    "    #tokenized_texts = []\n",
    "    true_labels = []\n",
    "    pred_labels = []\n",
    "\n",
    "    threshold = 0.5\n",
    "\n",
    "    total_eval_accuracy = 0\n",
    "    total_eval_loss = 0\n",
    "\n",
    "    for batch in validation_dataloader:\n",
    "        # print('val 1 free gpu',get_free_gpu())\n",
    "        b_input_key = batch[0]\n",
    "        b_labels = batch[1].to(device)\n",
    "\n",
    "\n",
    "        #convert key to text embedding\n",
    "        tk_batch = []\n",
    "        tk_batch_bf = []\n",
    "        #print('val batch',batch)\n",
    "        for t in b_input_key.detach().to('cpu').numpy():\n",
    "            tk, tk_bf = get_text_embedding(t[0], t[1], t[2], tokenizer, bert_model, para_map, para_len, wrd_len=wrd_len)\n",
    "            if tk.size()[0] == para_len:              \n",
    "                tk_batch.append(tk)\n",
    "                tk_batch_bf.append(tk_bf)\n",
    "            else:\n",
    "                print('token size error')\n",
    "                break\n",
    "            \n",
    "\n",
    "        tk_batch = torch.stack(tk_batch)\n",
    "        tk_batch = tk_batch.to(device)\n",
    "\n",
    "        tk_batch_bf = torch.stack(tk_batch_bf)\n",
    "        tk_batch_bf = tk_batch_bf.to(device)\n",
    "        # print('val 2 free gpu',get_free_gpu())\n",
    "\n",
    "        with torch.no_grad():\n",
    "\n",
    "            logits, x1, x2 = model(tk_batch, tk_batch_bf)\n",
    "            cos_sim = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "            sim = cos_sim(x1,x2)\n",
    "            sim = sim.reshape(-1,1)\n",
    "            #loss_func = BCELoss()\n",
    "            #val_loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "\n",
    "            tk_batch.detach().to('cpu')\n",
    "            del tk_batch\n",
    "            tk_batch_bf.detach().to('cpu')\n",
    "            del tk_batch_bf           \n",
    "            # print('val 3 free gpu',get_free_gpu())\n",
    "            \n",
    "            if class_weight != None:\n",
    "                pos_weight = torch.tensor(class_weight).to(device)\n",
    "                # weights = torch.tensor([pos_weight]).to(device)\n",
    "                ct_loss = nn.CrossEntropyLoss() #weight = weights\n",
    "                loss_func = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "            else:\n",
    "                ct_loss = nn.CrossEntropyLoss()\n",
    "                loss_func = BCEWithLogitsLoss()\n",
    "\n",
    "            global set_ct_loss\n",
    "            if set_ct_loss == True:\n",
    "                val_loss =  loss_func(logits,b_labels.type_as(logits)) \\\n",
    "                    -  ct_loss(sim, torch.argmax(b_labels,axis=1).type_as(sim).reshape(-1,1))  #convert labels to float for calculation\n",
    "            else: \n",
    "                val_loss =  loss_func(logits,b_labels.type_as(logits))\n",
    "\n",
    "            total_eval_loss += val_loss.item()\n",
    "\n",
    "            pred_label = torch.softmax(logits, dim=1)\n",
    "            b_labels = b_labels.to('cpu').numpy()\n",
    "            pred_label = pred_label.to('cpu').numpy()\n",
    "\n",
    "            #tokenized_texts.append(b_input_ids)\n",
    "            true_labels.append(b_labels)\n",
    "            pred_labels.append(pred_label)\n",
    "\n",
    "    # Flatten outputs\n",
    "    pred_labels = np.vstack(pred_labels)\n",
    "    true_labels = np.vstack(true_labels)\n",
    "\n",
    "    avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
    "\n",
    "    return  pred_labels, true_labels, avg_val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, num_labels, para_len, wrd_len, train_dataloader, validation_dataloader, model_path,\\\n",
    "                             optimizer=None, scheduler=None, epochs = 10, \\\n",
    "                             class_weight = None, patience = 5):\n",
    "\n",
    "    seed_val = 42\n",
    "\n",
    "    threshold = 0.5\n",
    "    #model_path = 'best_model.model'  # save the best model\n",
    "\n",
    "    random.seed(seed_val)\n",
    "    np.random.seed(seed_val)\n",
    "    torch.manual_seed(seed_val)\n",
    "    torch.cuda.manual_seed_all(seed_val)\n",
    "\n",
    "    training_stats = []\n",
    "\n",
    "    best_score = -0.5\n",
    "    best_epoch = 0\n",
    "    cnt = 0\n",
    "\n",
    "    total_t0 = time.time()\n",
    "\n",
    "    if optimizer == None:\n",
    "        optimizer = torch.optim.Adam(model.parameters(), lr=1e-5)\n",
    "\n",
    "    # For each epoch...\n",
    "    for epoch_i in range(0, epochs):\n",
    "\n",
    "        # ========================================\n",
    "        #               Training\n",
    "        # ========================================\n",
    "\n",
    "        # Perform one full pass over the training set.\n",
    "\n",
    "        print(\"\")\n",
    "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
    "        print('Training...')\n",
    "\n",
    "        # Measure how long the training epoch takes.\n",
    "        t0 = time.time()\n",
    "\n",
    "        # Reset the total loss for this epoch.\n",
    "        total_train_loss = 0\n",
    "        model.train()\n",
    "\n",
    "        # For each batch of training data...\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            \n",
    "            # `batch` contains three pytorch tensors:\n",
    "            #   [0]: (cik, fyear, fyear_bf)\n",
    "            #   [1]: labels\n",
    "            \n",
    "            # print('1 free gpu',get_free_gpu())\n",
    "            b_input_key = batch[0] # batch_size * (cik, fyear, fyear_bf)\n",
    "            b_labels = batch[1].to(device)\n",
    "            \n",
    "            \n",
    "            #convert key to text embedding\n",
    "            tk_batch = []\n",
    "            tk_batch_bf = []\n",
    "            #print('b_input_key',b_input_key)\n",
    "            time_start_tk = time.time()\n",
    "            for t in b_input_key.detach().to('cpu').numpy():\n",
    "                tk, tk_bf = get_text_embedding(t[0], t[1], t[2], tokenizer, bert_model, para_map, para_len, wrd_len=wrd_len)\n",
    "                if tk.size()[0] == para_len:              \n",
    "                    tk_batch.append(tk)\n",
    "                    tk_batch_bf.append(tk_bf)\n",
    "                    # print(len(tk_batch), len(tk_batch_bf))\n",
    "                else:\n",
    "                    print('token size error')\n",
    "                    break\n",
    "            # print(len(tk_batch), len(tk_batch_bf))\n",
    "            # print(\"----- token %s seconds -----\" % (time.time() - time_start_tk))\n",
    "                \n",
    "            tk_batch = torch.stack(tk_batch)\n",
    "            tk_batch = tk_batch.to(device)\n",
    "            \n",
    "            tk_batch_bf = torch.stack(tk_batch_bf)\n",
    "            tk_batch_bf = tk_batch_bf.to(device)\n",
    "            #  print('2 free gpu',get_free_gpu())\n",
    "            model.zero_grad()\n",
    "\n",
    "            time_start_batch_train = time.time()\n",
    "            logits, x1, x2 = model(tk_batch,tk_batch_bf)\n",
    "            cos_sim = nn.CosineSimilarity(dim=1, eps=1e-6)\n",
    "            sim = cos_sim(x1,x2)\n",
    "            sim = sim.reshape(-1,1)\n",
    "            #print(\"logits shape: \", b_input_ids.size(), b_labels.size(), logits.shape())\n",
    "            #loss_func = BCELoss()\n",
    "            #loss = loss_func(torch.sigmoid(logits.view(-1,num_labels)),b_labels.type_as(logits).view(-1,num_labels)) #convert labels to float for calculation\n",
    "\n",
    "            # add class weight\n",
    "            if class_weight != None:\n",
    "                pos_weight = torch.tensor(class_weight).to(device)\n",
    "                weights = torch.tensor([pos_weight]).to(device)\n",
    "                ct_loss = nn.CrossEntropyLoss()#weight = weights\n",
    "                loss_func = BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "            else:\n",
    "                ct_loss = nn.CrossEntropyLoss()\n",
    "                loss_func = BCEWithLogitsLoss()\n",
    "            \n",
    "            tk_batch.detach().to('cpu')\n",
    "            del tk_batch\n",
    "            tk_batch_bf.detach().to('cpu')\n",
    "            del tk_batch_bf\n",
    "            \n",
    "            # print('3 free gpu',get_free_gpu())\n",
    "            # print(logits.size(), b_labels.size())\n",
    "#             loss = loss_func(\n",
    "#                 logits.view(-1, num_labels),\n",
    "#                 b_labels.type_as(logits).view(\n",
    "#                     -1, num_labels))  \n",
    "            # convert labels to float for calculation\n",
    "            # global my_ct_loss, my_sim, my_label\n",
    "            # my_sim = sim\n",
    "            # my_label = b_labels\n",
    "            # my_ct_loss = ct_loss(sim, torch.argmax(b_labels,axis=1).type_as(logits).reshape(-1,1)) \n",
    "            global set_ct_loss\n",
    "            if set_ct_loss == True:\n",
    "                loss =  loss_func(logits,b_labels.type_as(logits)) \\\n",
    "                    -  ct_loss(sim, torch.argmax(b_labels,axis=1).type_as(sim).reshape(-1,1))  #convert labels to float for calculation\n",
    "            else: \n",
    "                loss =  loss_func(logits,b_labels.type_as(logits))\n",
    "\n",
    "             \n",
    "            total_train_loss += loss.item()\n",
    "\n",
    "            loss.backward()\n",
    "\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), 10.0)\n",
    "\n",
    "            optimizer.step()\n",
    "\n",
    "            # Update the learning rate.\n",
    "            if scheduler != None:\n",
    "                scheduler.step()\n",
    "\n",
    "        # Calculate the average loss over all of the batches.\n",
    "        avg_train_loss = total_train_loss / len(train_dataloader)\n",
    "\n",
    "        # Measure how long this epoch took.\n",
    "        training_time = time.time() - t0\n",
    "        print(\"Total training_time took {0:.2f} minutes \".format(training_time/60))\n",
    "\n",
    "            #print(\"\")\n",
    "            #print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
    "            #print(\"  Training epcoh took: {:}\".format(training_time))\n",
    "\n",
    "        # ========================================\n",
    "        #               Validation\n",
    "        # ========================================\n",
    "        # After the completion of each training epoch, measure our performance on\n",
    "        # our validation set.\n",
    "\n",
    "        testing = True\n",
    "\n",
    "        if testing:\n",
    "            print(\"\")\n",
    "            print(\"Running Validation...\")\n",
    "\n",
    "            t1 = time.time()\n",
    "\n",
    "            # Put the model in evaluation mode--the dropout layers behave differently\n",
    "            # during evaluation.\n",
    "            model.eval()\n",
    "\n",
    "            pred_labels, true_labels, avg_val_loss = model_eval(\n",
    "                model,  validation_dataloader, num_labels, class_weight=class_weight)\n",
    "\n",
    "            global val_label_save\n",
    "            global val_true_label_save\n",
    "            val_label_save.append(pred_labels)\n",
    "            val_true_label_save.append(true_labels)\n",
    "\n",
    "\n",
    "            pred_bools = np.argmax(pred_labels, axis=1)\n",
    "            true_bools = np.argmax(true_labels, axis=1)\n",
    "\n",
    "            val_f1 = f1_score(true_bools, pred_bools, average=None) * 100\n",
    "            val_f1 = val_f1[1]  # return f1 for  class 1\n",
    "            val_acc = (pred_bools == true_bools).astype(int).sum() / len(pred_bools)\n",
    "            val_auc = roc_auc_score(true_bools, pred_labels[:,1])\n",
    "\n",
    "            #print('Validation Accuracy: {0:.4f}, F1: {1:.4f}, Loss: {2:.4f}'.format(val_f1, val_acc, avg_val_loss))\n",
    "            #print(classification_report(np.array(true_labels), pred_bools, target_names=label_cols) )\n",
    "            print(\"Epoch {0}\\t Train Loss: {1:.4f}\\t Val Loss {2:.4f}\\t Val Acc: {3:.4f}\\t Val F1: {4:.4f}\\t Val AUC: {5:.4f}\".\\\n",
    "                format(epoch_i +1, avg_train_loss, avg_val_loss, val_acc, val_f1, val_auc))\n",
    "\n",
    "            # Measure how long the validation run took.\n",
    "            validation_time = time.time() - t1\n",
    "            print(\"Total val_time took {0:.2f} minutes \".format(validation_time/60))\n",
    "\n",
    "            #print(\"  Validation Loss: {0:.2f}\".format(val_f1_accuracy))\n",
    "            #print(\"  Validation took: {:}\".format(validation_time))\n",
    "\n",
    "            # Record all statistics from this epoch.\n",
    "            training_stats.append({\n",
    "                'epoch': epoch_i + 1,\n",
    "                'Training Loss': avg_train_loss,\n",
    "                'Valid. Loss': avg_val_loss,\n",
    "                'Valid. Accur.': val_f1,\n",
    "                'Valid. AUC':val_auc,\n",
    "                'Best F1': best_score,\n",
    "                'Best epoch': best_epoch\n",
    "                #'Training Time': training_time,\n",
    "                #'Validation Time': validation_time\n",
    "            })\n",
    "\n",
    "            # early stopping\n",
    "            if val_f1 > best_score:\n",
    "                best_score = val_f1\n",
    "                best_epoch = epoch_i + 1\n",
    "                torch.save(copy.deepcopy(model.state_dict()), model_path)\n",
    "                print(\"model saved\")\n",
    "                cnt = 0\n",
    "            else:\n",
    "                cnt += 1\n",
    "                if cnt == patience:\n",
    "                    print(\"\\n\")\n",
    "                    print(\"early stopping at epoch {0}\".format(epoch_i + 1))\n",
    "                    break\n",
    "\n",
    "            print(\"\")\n",
    "            #print(\"Training complete!\")\n",
    "\n",
    "            print(\"Total training took {0:.2f} minutes\".format((time.time()-total_t0)/60))\n",
    "        else:\n",
    "            training_stats = 0\n",
    "            print(avg_train_loss)\n",
    "        \n",
    "    return model, training_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 2 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce GTX 1080 Ti\n",
      "1\n",
      "(168, 5)\n",
      "successfully load data ...\n",
      "Loading BERT tokenizer...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertModel: ['cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "------------\n",
      "fraud\n",
      "------------\n",
      "\n",
      "fold 0 \n",
      "\n",
      "train fraud 14 test fraud 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======== Epoch 1 / 20 ========\n",
      "Training...\n",
      "Total training_time took 0.86 minutes \n",
      "\n",
      "Running Validation...\n",
      "Epoch 1\t Train Loss: 2.3960\t Val Loss 2.1534\t Val Acc: 0.7500\t Val F1: 0.0000\t Val AUC: 0.2240\n",
      "Total val_time took 0.40 minutes \n",
      "model saved\n",
      "\n",
      "Total training took 1.38 minutes\n",
      "\n",
      "======== Epoch 2 / 20 ========\n",
      "Training...\n",
      "Total training_time took 0.86 minutes \n",
      "\n",
      "Running Validation...\n",
      "Epoch 2\t Train Loss: 1.9844\t Val Loss 2.1862\t Val Acc: 0.1071\t Val F1: 13.7931\t Val AUC: 0.2526\n",
      "Total val_time took 0.40 minutes \n",
      "model saved\n",
      "\n",
      "Total training took 2.75 minutes\n",
      "\n",
      "======== Epoch 3 / 20 ========\n",
      "Training...\n",
      "Total training_time took 0.86 minutes \n",
      "\n",
      "Running Validation...\n",
      "Epoch 3\t Train Loss: 1.5767\t Val Loss 2.2496\t Val Acc: 0.1786\t Val F1: 14.8148\t Val AUC: 0.2630\n",
      "Total val_time took 0.40 minutes \n",
      "model saved\n",
      "\n",
      "Total training took 4.13 minutes\n",
      "\n",
      "======== Epoch 4 / 20 ========\n",
      "Training...\n",
      "Total training_time took 0.86 minutes \n",
      "\n",
      "Running Validation...\n",
      "Epoch 4\t Train Loss: 1.1645\t Val Loss 2.4469\t Val Acc: 0.2321\t Val F1: 15.6863\t Val AUC: 0.2839\n",
      "Total val_time took 0.41 minutes \n",
      "model saved\n",
      "\n",
      "Total training took 5.52 minutes\n",
      "\n",
      "======== Epoch 5 / 20 ========\n",
      "Training...\n",
      "Total training_time took 0.93 minutes \n",
      "\n",
      "Running Validation...\n",
      "Epoch 5\t Train Loss: 1.0489\t Val Loss 2.5177\t Val Acc: 0.3214\t Val F1: 13.6364\t Val AUC: 0.2891\n",
      "Total val_time took 0.44 minutes \n",
      "\n",
      "Total training took 6.89 minutes\n",
      "\n",
      "======== Epoch 6 / 20 ========\n",
      "Training...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 137\u001b[0m\n\u001b[1;32m    132\u001b[0m model \u001b[39m=\u001b[39m simple_siamese(emb_dim, wrd_len, num_filters, kernel_sizes,\\\n\u001b[1;32m    133\u001b[0m                     kernel_sizes2, num_classes\u001b[39m=\u001b[39mnum_classes, dropout_rate \u001b[39m=\u001b[39m dropout_rate)\n\u001b[1;32m    134\u001b[0m model\u001b[39m.\u001b[39mto(device)\n\u001b[0;32m--> 137\u001b[0m model, training_stats \u001b[39m=\u001b[39m train_model(model, num_classes, para_len, wrd_len, train_dataloader, validation_dataloader, \\\n\u001b[1;32m    138\u001b[0m                                                 model_path \u001b[39m=\u001b[39;49m model_name, class_weight \u001b[39m=\u001b[39;49m class_weight,\\\n\u001b[1;32m    139\u001b[0m                                                 optimizer\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, scheduler\u001b[39m=\u001b[39;49m\u001b[39mNone\u001b[39;49;00m, epochs \u001b[39m=\u001b[39;49m \u001b[39m20\u001b[39;49m)\n\u001b[1;32m    141\u001b[0m \u001b[39mprint\u001b[39m(\u001b[39m\"\u001b[39m\u001b[39mload the best model ... \u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    143\u001b[0m model\u001b[39m.\u001b[39mload_state_dict(torch\u001b[39m.\u001b[39mload(model_name))\n",
      "Cell \u001b[0;32mIn[8], line 64\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(model, num_labels, para_len, wrd_len, train_dataloader, validation_dataloader, model_path, optimizer, scheduler, epochs, class_weight, patience)\u001b[0m\n\u001b[1;32m     62\u001b[0m time_start_tk \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m     63\u001b[0m \u001b[39mfor\u001b[39;00m t \u001b[39min\u001b[39;00m b_input_key\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39m'\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m'\u001b[39m)\u001b[39m.\u001b[39mnumpy():\n\u001b[0;32m---> 64\u001b[0m     tk, tk_bf \u001b[39m=\u001b[39m get_text_embedding(t[\u001b[39m0\u001b[39;49m], t[\u001b[39m1\u001b[39;49m], t[\u001b[39m2\u001b[39;49m], tokenizer, bert_model, para_map, para_len, wrd_len\u001b[39m=\u001b[39;49mwrd_len)\n\u001b[1;32m     65\u001b[0m     \u001b[39mif\u001b[39;00m tk\u001b[39m.\u001b[39msize()[\u001b[39m0\u001b[39m] \u001b[39m==\u001b[39m para_len:              \n\u001b[1;32m     66\u001b[0m         tk_batch\u001b[39m.\u001b[39mappend(tk)\n",
      "Cell \u001b[0;32mIn[3], line 26\u001b[0m, in \u001b[0;36mget_text_embedding\u001b[0;34m(cik, fyear, fyear_bf, tokenizer, bert_model, para_map, para_len, wrd_len)\u001b[0m\n\u001b[1;32m     23\u001b[0m     token_embeddings \u001b[39m=\u001b[39m token_embeddings\n\u001b[1;32m     25\u001b[0m \u001b[39m#get embedding for input_bf\u001b[39;00m\n\u001b[0;32m---> 26\u001b[0m token_embeddings_bf, masks_bf \u001b[39m=\u001b[39m get_pretrained_wordvector(input_bf, tokenizer, bert_model, max_len \u001b[39m=\u001b[39;49m wrd_len)\n\u001b[1;32m     27\u001b[0m token_embeddings_bf \u001b[39m=\u001b[39m token_embeddings_bf\u001b[39m.\u001b[39mto(device) \u001b[39m*\u001b[39m masks_bf\u001b[39m.\u001b[39munsqueeze(\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mto(device) \u001b[39m# (atc_num_para, #wrd_len, #dim)\u001b[39;00m\n\u001b[1;32m     28\u001b[0m \u001b[39m# padding paragraphs\u001b[39;00m\n\u001b[1;32m     29\u001b[0m \u001b[39m# print('1 token_embeddings_bf',token_embeddings_bf.size())\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 16\u001b[0m, in \u001b[0;36mget_pretrained_wordvector\u001b[0;34m(sentences, tokenizer, bert_model, max_len)\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[39m# For every sentence...\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m sent \u001b[39min\u001b[39;00m sentences:\n\u001b[1;32m      9\u001b[0m \u001b[39m# `encode_plus` will:\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[39m#   (1) Tokenize the sentence.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[39m#   (5) Pad or truncate the sentence to `max_length`\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[39m#   (6) Create attention masks for [PAD] tokens.\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m     encoded_dict \u001b[39m=\u001b[39m tokenizer\u001b[39m.\u001b[39;49mencode_plus(\n\u001b[1;32m     17\u001b[0m                     sent,                      \u001b[39m# Sentence to encode.\u001b[39;49;00m\n\u001b[1;32m     18\u001b[0m                     add_special_tokens \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m, \u001b[39m# Add '[CLS]' and '[SEP]'\u001b[39;49;00m\n\u001b[1;32m     19\u001b[0m                     max_length \u001b[39m=\u001b[39;49m max_len,           \u001b[39m# Pad & truncate all sentences.\u001b[39;49;00m\n\u001b[1;32m     20\u001b[0m                     pad_to_max_length \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,\n\u001b[1;32m     21\u001b[0m                     \u001b[39m#padding='max_length',\u001b[39;49;00m\n\u001b[1;32m     22\u001b[0m                     return_attention_mask \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m,   \u001b[39m# Construct attn. masks.\u001b[39;49;00m\n\u001b[1;32m     23\u001b[0m                     return_tensors \u001b[39m=\u001b[39;49m \u001b[39m'\u001b[39;49m\u001b[39mpt\u001b[39;49m\u001b[39m'\u001b[39;49m,     \u001b[39m# Return pytorch tensors.\u001b[39;49;00m\n\u001b[1;32m     24\u001b[0m                )\n\u001b[1;32m     26\u001b[0m     \u001b[39m# Add the encoded sentence to the list.    \u001b[39;00m\n\u001b[1;32m     27\u001b[0m     input_ids\u001b[39m.\u001b[39mappend(encoded_dict[\u001b[39m'\u001b[39m\u001b[39minput_ids\u001b[39m\u001b[39m'\u001b[39m])\n",
      "File \u001b[0;32m~/.conda/envs/jujun_env/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:2699\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding, truncation, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m   2689\u001b[0m \u001b[39m# Backward compatibility for 'truncation_strategy', 'pad_to_max_length'\u001b[39;00m\n\u001b[1;32m   2690\u001b[0m padding_strategy, truncation_strategy, max_length, kwargs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_get_padding_truncation_strategies(\n\u001b[1;32m   2691\u001b[0m     padding\u001b[39m=\u001b[39mpadding,\n\u001b[1;32m   2692\u001b[0m     truncation\u001b[39m=\u001b[39mtruncation,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   2696\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   2697\u001b[0m )\n\u001b[0;32m-> 2699\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_encode_plus(\n\u001b[1;32m   2700\u001b[0m     text\u001b[39m=\u001b[39;49mtext,\n\u001b[1;32m   2701\u001b[0m     text_pair\u001b[39m=\u001b[39;49mtext_pair,\n\u001b[1;32m   2702\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m   2703\u001b[0m     padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m   2704\u001b[0m     truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m   2705\u001b[0m     max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m   2706\u001b[0m     stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m   2707\u001b[0m     is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m   2708\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m   2709\u001b[0m     return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m   2710\u001b[0m     return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m   2711\u001b[0m     return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m   2712\u001b[0m     return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m   2713\u001b[0m     return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m   2714\u001b[0m     return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m   2715\u001b[0m     return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m   2716\u001b[0m     verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m   2717\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m   2718\u001b[0m )\n",
      "File \u001b[0;32m~/.conda/envs/jujun_env/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:502\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._encode_plus\u001b[0;34m(self, text, text_pair, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose, **kwargs)\u001b[0m\n\u001b[1;32m    479\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_encode_plus\u001b[39m(\n\u001b[1;32m    480\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m    481\u001b[0m     text: Union[TextInput, PreTokenizedInput],\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    498\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m    499\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m BatchEncoding:\n\u001b[1;32m    501\u001b[0m     batched_input \u001b[39m=\u001b[39m [(text, text_pair)] \u001b[39mif\u001b[39;00m text_pair \u001b[39melse\u001b[39;00m [text]\n\u001b[0;32m--> 502\u001b[0m     batched_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_encode_plus(\n\u001b[1;32m    503\u001b[0m         batched_input,\n\u001b[1;32m    504\u001b[0m         is_split_into_words\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    505\u001b[0m         add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    506\u001b[0m         padding_strategy\u001b[39m=\u001b[39;49mpadding_strategy,\n\u001b[1;32m    507\u001b[0m         truncation_strategy\u001b[39m=\u001b[39;49mtruncation_strategy,\n\u001b[1;32m    508\u001b[0m         max_length\u001b[39m=\u001b[39;49mmax_length,\n\u001b[1;32m    509\u001b[0m         stride\u001b[39m=\u001b[39;49mstride,\n\u001b[1;32m    510\u001b[0m         pad_to_multiple_of\u001b[39m=\u001b[39;49mpad_to_multiple_of,\n\u001b[1;32m    511\u001b[0m         return_tensors\u001b[39m=\u001b[39;49mreturn_tensors,\n\u001b[1;32m    512\u001b[0m         return_token_type_ids\u001b[39m=\u001b[39;49mreturn_token_type_ids,\n\u001b[1;32m    513\u001b[0m         return_attention_mask\u001b[39m=\u001b[39;49mreturn_attention_mask,\n\u001b[1;32m    514\u001b[0m         return_overflowing_tokens\u001b[39m=\u001b[39;49mreturn_overflowing_tokens,\n\u001b[1;32m    515\u001b[0m         return_special_tokens_mask\u001b[39m=\u001b[39;49mreturn_special_tokens_mask,\n\u001b[1;32m    516\u001b[0m         return_offsets_mapping\u001b[39m=\u001b[39;49mreturn_offsets_mapping,\n\u001b[1;32m    517\u001b[0m         return_length\u001b[39m=\u001b[39;49mreturn_length,\n\u001b[1;32m    518\u001b[0m         verbose\u001b[39m=\u001b[39;49mverbose,\n\u001b[1;32m    519\u001b[0m         \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[1;32m    522\u001b[0m     \u001b[39m# Return tensor is None, then we can remove the leading batch axis\u001b[39;00m\n\u001b[1;32m    523\u001b[0m     \u001b[39m# Overflowing tokens are returned as a batch of output so we keep them in this case\u001b[39;00m\n\u001b[1;32m    524\u001b[0m     \u001b[39mif\u001b[39;00m return_tensors \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m return_overflowing_tokens:\n",
      "File \u001b[0;32m~/.conda/envs/jujun_env/lib/python3.11/site-packages/transformers/tokenization_utils_fast.py:429\u001b[0m, in \u001b[0;36mPreTrainedTokenizerFast._batch_encode_plus\u001b[0;34m(self, batch_text_or_text_pairs, add_special_tokens, padding_strategy, truncation_strategy, max_length, stride, is_split_into_words, pad_to_multiple_of, return_tensors, return_token_type_ids, return_attention_mask, return_overflowing_tokens, return_special_tokens_mask, return_offsets_mapping, return_length, verbose)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[39m# Set the truncation and padding strategy and restore the initial configuration\u001b[39;00m\n\u001b[1;32m    421\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mset_truncation_and_padding(\n\u001b[1;32m    422\u001b[0m     padding_strategy\u001b[39m=\u001b[39mpadding_strategy,\n\u001b[1;32m    423\u001b[0m     truncation_strategy\u001b[39m=\u001b[39mtruncation_strategy,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    426\u001b[0m     pad_to_multiple_of\u001b[39m=\u001b[39mpad_to_multiple_of,\n\u001b[1;32m    427\u001b[0m )\n\u001b[0;32m--> 429\u001b[0m encodings \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_tokenizer\u001b[39m.\u001b[39;49mencode_batch(\n\u001b[1;32m    430\u001b[0m     batch_text_or_text_pairs,\n\u001b[1;32m    431\u001b[0m     add_special_tokens\u001b[39m=\u001b[39;49madd_special_tokens,\n\u001b[1;32m    432\u001b[0m     is_pretokenized\u001b[39m=\u001b[39;49mis_split_into_words,\n\u001b[1;32m    433\u001b[0m )\n\u001b[1;32m    435\u001b[0m \u001b[39m# Convert encoding to dict\u001b[39;00m\n\u001b[1;32m    436\u001b[0m \u001b[39m# `Tokens` has type: Tuple[\u001b[39;00m\n\u001b[1;32m    437\u001b[0m \u001b[39m#                       List[Dict[str, List[List[int]]]] or List[Dict[str, 2D-Tensor]],\u001b[39;00m\n\u001b[1;32m    438\u001b[0m \u001b[39m#                       List[EncodingFast]\u001b[39;00m\n\u001b[1;32m    439\u001b[0m \u001b[39m#                    ]\u001b[39;00m\n\u001b[1;32m    440\u001b[0m \u001b[39m# with nested dimensions corresponding to batch, overflows, sequence length\u001b[39;00m\n\u001b[1;32m    441\u001b[0m tokens_and_encodings \u001b[39m=\u001b[39m [\n\u001b[1;32m    442\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_convert_encoding(\n\u001b[1;32m    443\u001b[0m         encoding\u001b[39m=\u001b[39mencoding,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    452\u001b[0m     \u001b[39mfor\u001b[39;00m encoding \u001b[39min\u001b[39;00m encodings\n\u001b[1;32m    453\u001b[0m ]\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # If there's a GPU available...\n",
    "    if torch.cuda.is_available():    \n",
    "        # Tell PyTorch to use the GPU. \n",
    "        id = 1 \n",
    "        torch.cuda.set_device(1)\n",
    "        device = torch.device(\"cuda\")\n",
    "        print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
    "        print('We will use the GPU:', torch.cuda.get_device_name(id))\n",
    "        print(torch.cuda.current_device())\n",
    "    # If not...\n",
    "    else:\n",
    "        print('No GPU available, using the CPU instead.')\n",
    "        device = torch.device(\"cpu\")\n",
    "\n",
    "    # print(get_free_gpu())\n",
    "\n",
    "    # load data\n",
    "    para_map = pickle.load(open(\"/research/rliu/fraud/data/mda/paragraphs_1994_2016.pkl\",\"rb\"))\n",
    "    pos_neg_pair = pd.read_csv('./data/pos_neg_pair.csv')\n",
    "    pos_neg_pair = pos_neg_pair.dropna()\n",
    "\n",
    "\n",
    "    # pos_index = pos_neg_pair[pos_neg_pair.fraud == 1].index[0:5]\n",
    "    # neg_index = pos_neg_pair[pos_neg_pair.fraud == 0].sample(len(pos_index)).index\n",
    "    # df = pos_neg_pair.loc[neg_index.append(pos_index),:]\n",
    "    # print(df.shape)\n",
    "    pos_cik = list(set(pos_neg_pair[pos_neg_pair.fraud == 1].cik))\n",
    "    neg_cik = list(set(pos_neg_pair[pos_neg_pair.fraud == 0].cik))\n",
    "    neg_cik = [c for c in neg_cik if c not in pos_cik]\n",
    "    neg_cik = random.sample(neg_cik, len(pos_cik))\n",
    "    df = pos_neg_pair[pos_neg_pair.cik.isin(pos_cik[0:10] + neg_cik[0:10])]\n",
    "    print(df.shape)\n",
    "    print('successfully load data ...')\n",
    "    \n",
    "\n",
    "\n",
    "    emb_dim = 768\n",
    "    wrd_len = 50 #100\n",
    "    para_len = 30 #60\n",
    "    num_filters = 128\n",
    "    kernel_sizes =  (1,10)\n",
    "    kernel_sizes2 = (5,3) # (1,1) #(2,2)\n",
    "    dropout_rate = 0.3\n",
    "    num_classes=2.0\n",
    "    batch_size = 16\n",
    "    para_map = para_map\n",
    "    class_weight = 5\n",
    "\n",
    "    set_ct_loss = False\n",
    "    result = []\n",
    "    val_label_save = []\n",
    "    val_true_label_save = []\n",
    "    label_cols = ['fraud']\n",
    "\n",
    "    # global my_ct_loss\n",
    "    # global my_sim\n",
    "    # global my_label\n",
    "\n",
    "    #embedding\n",
    "    print('Loading BERT tokenizer...')\n",
    "    tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)\n",
    "\n",
    "\n",
    "    bert_model = AutoModel.from_pretrained(\n",
    "        # 'ProsusAI/finbert',\n",
    "        'bert-base-uncased',\n",
    "        # 'yiyanghkust/finbert-pretrain',\n",
    "        num_labels = 2, \n",
    "        output_attentions = False, # Whether the model returns attentions weights.\n",
    "        output_hidden_states = True, # Whether the model returns all hidden-states.\n",
    "        )\n",
    "    bert_model.cuda()\n",
    "\n",
    "    for col in label_cols:\n",
    "        print(\"\\n------------\")\n",
    "        print(col)\n",
    "        print(\"------------\")\n",
    "\n",
    "        y = df[col].astype(int).values\n",
    "        x_key = df[['cik', 'fyear', 'fyear_bf']].values\n",
    "\n",
    "        fold = 0\n",
    "\n",
    "        skf = StratifiedKFold(n_splits=3, random_state=0, shuffle=True)\n",
    "\n",
    "        for train_index, test_index in skf.split(x_key, y):\n",
    "\n",
    "            print(\"\\nfold {} \\n\".format(fold))\n",
    "\n",
    "            fold += 1\n",
    "            X_train, X_test = x_key[train_index], x_key[test_index]\n",
    "            X_train = torch.tensor(X_train)\n",
    "            X_test = torch.tensor(X_test)\n",
    "\n",
    "            Y_train, Y_test = y[train_index], y[test_index]\n",
    "            print('train fraud', sum(Y_train),'test fraud', sum(Y_test))\n",
    "\n",
    "            Y_train = pd.get_dummies(Y_train).values\n",
    "            Y_train = torch.tensor(Y_train)\n",
    "\n",
    "            Y_test = pd.get_dummies(Y_test).values\n",
    "            Y_test = torch.tensor(Y_test)\n",
    "\n",
    "            train_dataset = TensorDataset(X_train, Y_train)\n",
    "            val_dataset = TensorDataset(X_test, Y_test)\n",
    "\n",
    "            train_dataloader = DataLoader(\n",
    "                train_dataset,  # The training samples.\n",
    "                sampler=RandomSampler(train_dataset),  # Select batches randomly\n",
    "                batch_size=batch_size  # Trains with this batch size.\n",
    "            )\n",
    "\n",
    "            validation_dataloader = DataLoader(\n",
    "                val_dataset,  # The validation samples.\n",
    "                sampler=RandomSampler(\n",
    "                    val_dataset),  # Pull out batches sequentially.\n",
    "                batch_size=batch_size  # Evaluate with this batch size.\n",
    "            )\n",
    "\n",
    "            if class_weight == None:\n",
    "                pass\n",
    "            else:\n",
    "                train_sample_weight = np.array(\n",
    "                    [class_weight if i[1] == 1 else 1 for i in Y_train])\n",
    "                test_sample_weight = np.array(\n",
    "                    [class_weight if i[1] == 1 else 1 for i in Y_test])\n",
    "\n",
    "            model_name = \"./model/simple_siamese_\" + str(fold)\n",
    "            #model = cnn(emb_dim, seq_len, num_filters, kernel_sizes, num_labels)\n",
    "            model = simple_siamese(emb_dim, wrd_len, num_filters, kernel_sizes,\\\n",
    "                                kernel_sizes2, num_classes=num_classes, dropout_rate = dropout_rate)\n",
    "            model.to(device)\n",
    "\n",
    "\n",
    "            model, training_stats = train_model(model, num_classes, para_len, wrd_len, train_dataloader, validation_dataloader, \\\n",
    "                                                            model_path = model_name, class_weight = class_weight,\\\n",
    "                                                            optimizer=None, scheduler=None, epochs = 20)\n",
    "\n",
    "            print(\"load the best model ... \")\n",
    "\n",
    "            model.load_state_dict(torch.load(model_name))\n",
    "\n",
    "            # show performance of best model\n",
    "            model.eval()\n",
    "            pred_labels, true_labels,avg_val_loss = model_eval(model, \\\n",
    "                                                    validation_dataloader, num_classes, class_weight = class_weight)\n",
    "\n",
    "            pred_bools = np.argmax(pred_labels, axis = 1)\n",
    "            print(\"np.argmax for pred_labels\", pred_labels)__\n",
    "            true_bools = np.argmax(true_labels, axis = 1)\n",
    "            print(\"np.argmax for true_labels\", true_labels)\n",
    "\n",
    "            p, r, f, _ = precision_recall_fscore_support(true_bools,pred_bools, pos_label = 1)\n",
    "            #val_f1 = f1_score(true_bools,pred_bools, average = None)*100\n",
    "            #val_f1 = val_f1[1] # return f1 for  class 1\n",
    "            val_acc = (pred_bools == true_bools).astype(int).sum()/len(pred_bools)\n",
    "            val_auc = roc_auc_score(true_bools, pred_labels[:,1])\n",
    "\n",
    "            print('Precision: {0:.4f}, Recall: {1:.4f}, F1: {2:.4f}, Loss: {3:.4f}, AUC: {4:.4f}'.format(p[1], r[1], f[1], avg_val_loss, val_auc))\n",
    "            print(classification_report(true_bools, pred_bools) )\n",
    "\n",
    "\n",
    "            result.append([col, fold, p[1], r[1], f[1], val_acc, val_auc,training_stats[-1][\"Best epoch\"]])\n",
    "            with open(\"./result/simple_siamese.pkl\", \"wb\") as fp:   #Pickling\n",
    "                pickle.dump(result, fp)\n",
    "            \n",
    "            torch.cuda.empty_cache()\n",
    "            get_free_gpu()\n",
    "    print('=== finish  === ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_label_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.3543, 0.6457],\n",
       "        [0.4256, 0.5744]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.softmax(torch.tensor([[1.2,1.8],[0.4,0.7]]),dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_true_label_save)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68, 2)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_label_save[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(68,)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b = val_label_save[0].sum(axis=1)\n",
    "b.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.17065549, 0.9254408 , 0.1373634 , 1.1494069 , 0.9584285 ,\n",
       "       0.7434721 , 0.71889096, 1.6119001 , 1.0690413 , 1.3655858 ,\n",
       "       1.1838102 , 1.1821411 , 1.3799114 , 1.0219538 , 0.7723524 ,\n",
       "       0.5331135 , 0.78861094, 0.20119976, 0.91680825, 1.2117441 ,\n",
       "       0.98372537, 1.0496196 , 0.23555559, 0.92041314, 0.2754027 ,\n",
       "       0.04913634, 0.9315622 , 0.6531485 , 1.0037191 , 0.6413631 ,\n",
       "       0.5984663 , 1.0029293 , 0.2601212 , 1.0076905 , 0.96777105,\n",
       "       0.63611203, 0.9037967 , 0.15120733, 0.35794693, 1.3399355 ,\n",
       "       0.9347809 , 1.40305   , 0.31755784, 0.9960414 , 0.9622948 ,\n",
       "       0.4154457 , 1.        , 0.99579954, 0.7503063 , 1.1963104 ,\n",
       "       0.71727896, 0.3954625 , 0.65868986, 1.1041486 , 0.9916811 ,\n",
       "       0.8508855 , 1.1074402 , 0.5861236 , 0.74051785, 1.2596555 ,\n",
       "       1.1963104 , 1.        , 0.16252139, 1.0134362 , 0.99513257,\n",
       "       0.9999999 , 0.9817808 , 1.0614734 , 0.79483014, 0.6581383 ,\n",
       "       1.0754023 , 0.5320103 , 1.4135199 , 0.99123055], dtype=float32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = val_label_save[-6][:,1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.58041525, 0.5365413 , 0.7059011 , 0.52205956, 0.6233735 ,\n",
       "       0.88234216, 0.86013997, 0.97542566, 0.8918372 , 0.99714714,\n",
       "       0.832678  , 0.62536   , 0.92515063, 0.99168956, 0.9210675 ,\n",
       "       0.68310183, 0.5652087 , 0.64614487, 0.9021347 , 0.9999063 ,\n",
       "       0.88040686, 0.99464405, 0.9999999 , 0.76855993, 0.91110325,\n",
       "       0.7193723 ], dtype=float32)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a[a>0.5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data\n",
    "para_map = pickle.load(open(\"/research/rliu/fraud/data/mda/paragraphs_1994_2016.pkl\",\"rb\"))\n",
    "pos_neg_pair = pd.read_csv('./data/pos_neg_pair.csv')\n",
    "pos_neg_pair = pos_neg_pair.dropna()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "75208      12\n",
       "803014     10\n",
       "849547     10\n",
       "6284       10\n",
       "859475     10\n",
       "           ..\n",
       "928395      1\n",
       "932112      1\n",
       "18498       1\n",
       "947431      1\n",
       "1604028     1\n",
       "Name: cik, Length: 328, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_neg_pair[pos_neg_pair.fraud == 1].cik.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cik</th>\n",
       "      <th>fyear</th>\n",
       "      <th>count_para</th>\n",
       "      <th>fraud</th>\n",
       "      <th>fyear_bf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>150242</th>\n",
       "      <td>1604028</td>\n",
       "      <td>2015</td>\n",
       "      <td>183</td>\n",
       "      <td>1.0</td>\n",
       "      <td>2014.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150243</th>\n",
       "      <td>1604028</td>\n",
       "      <td>2016</td>\n",
       "      <td>152</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2015.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            cik  fyear  count_para  fraud  fyear_bf\n",
       "150242  1604028   2015         183    1.0    2014.0\n",
       "150243  1604028   2016         152    0.0    2015.0"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pos_neg_pair[pos_neg_pair.cik == 1604028]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_cik = list(set(pos_neg_pair[pos_neg_pair.fraud == 1].cik))\n",
    "neg_cik = list(set(pos_neg_pair[pos_neg_pair.fraud == 0].cik))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "neg_cik = [c for c in neg_cik if c not in pos_cik]\n",
    "neg_cik = random.sample(neg_cik, len(pos_cik))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "328 328\n"
     ]
    }
   ],
   "source": [
    "print(len(neg_cik), len(pos_cik))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "656"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(pos_cik + neg_cik)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "successfully load data ...\n",
      "(6284, 5)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print('successfully load data ...')\n",
    "df = pos_neg_pair[pos_neg_pair.cik.isin(pos_cik + neg_cik)]\n",
    "print(df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "656"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(set(df.cik))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0]], device='cuda:1', dtype=torch.uint8)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0], device='cuda:1')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_label_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_tb = pd.DataFrame(result, columns=['label','fold','precison','recall','f1','acc','auc','best_epoch'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([2, 3])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor([2,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pos_weight = 10\n",
    "weights = torch.tensor([1,pos_weight]).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_loss = nn.CrossEntropyLoss(weight = weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ct_loss = nn.CrossEntropyLoss(weight = weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0],\n",
       "        [0, 1],\n",
       "        [1, 0],\n",
       "        [1, 0]], device='cuda:1', dtype=torch.uint8)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "my_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "cross_entropy: weight tensor should be defined either for all 1 classes or no classes but got weight tensor of shape: [2]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[29], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m ct_loss(my_sim\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m), torch\u001b[39m.\u001b[39;49margmax(my_label,axis\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m)\u001b[39m.\u001b[39;49mtype_as(my_sim)\u001b[39m.\u001b[39;49mreshape(\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m,\u001b[39m1\u001b[39;49m))\n",
      "File \u001b[0;32m~/.conda/envs/jujun_env/lib/python3.11/site-packages/torch/nn/modules/module.py:1194\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1190\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1191\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1192\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1193\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1194\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49m\u001b[39minput\u001b[39;49m, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   1195\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1196\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[0;32m~/.conda/envs/jujun_env/lib/python3.11/site-packages/torch/nn/modules/loss.py:1174\u001b[0m, in \u001b[0;36mCrossEntropyLoss.forward\u001b[0;34m(self, input, target)\u001b[0m\n\u001b[1;32m   1173\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor, target: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[0;32m-> 1174\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mcross_entropy(\u001b[39minput\u001b[39;49m, target, weight\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight,\n\u001b[1;32m   1175\u001b[0m                            ignore_index\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mignore_index, reduction\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mreduction,\n\u001b[1;32m   1176\u001b[0m                            label_smoothing\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mlabel_smoothing)\n",
      "File \u001b[0;32m~/.conda/envs/jujun_env/lib/python3.11/site-packages/torch/nn/functional.py:3026\u001b[0m, in \u001b[0;36mcross_entropy\u001b[0;34m(input, target, weight, size_average, ignore_index, reduce, reduction, label_smoothing)\u001b[0m\n\u001b[1;32m   3024\u001b[0m \u001b[39mif\u001b[39;00m size_average \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mor\u001b[39;00m reduce \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m   3025\u001b[0m     reduction \u001b[39m=\u001b[39m _Reduction\u001b[39m.\u001b[39mlegacy_get_string(size_average, reduce)\n\u001b[0;32m-> 3026\u001b[0m \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39;49m_C\u001b[39m.\u001b[39;49m_nn\u001b[39m.\u001b[39;49mcross_entropy_loss(\u001b[39minput\u001b[39;49m, target, weight, _Reduction\u001b[39m.\u001b[39;49mget_enum(reduction), ignore_index, label_smoothing)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: cross_entropy: weight tensor should be defined either for all 1 classes or no classes but got weight tensor of shape: [2]"
     ]
    }
   ],
   "source": [
    "ct_loss(my_sim.reshape(-1,1), torch.argmax(my_label,axis=1).type_as(my_sim).reshape(-1,1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "jujun_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0 | packaged by conda-forge | (main, Jan 14 2023, 12:27:40) [GCC 11.3.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "d855bc81b0407a867dfce63c14005faefd0874fe81a6ad417f5d7e68a600d1a7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
